{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f2c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created results directory structure in ../results/03_basic_models\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from arch import arch_model\n",
    "from scipy import stats\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import sys\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.scan import scan\n",
    "\n",
    "sys.path.append('../src')\n",
    "from utils import (\n",
    "    train_test_split_timeseries,\n",
    "    extract_stock_returns,\n",
    "    create_results_directory,\n",
    "    print_convergence_diagnostics,\n",
    "    plot_mcmc_diagnostics\n",
    ")\n",
    "from priors import AR1_priors, GARCH_priors,SV_priors\n",
    "\n",
    "np.random.seed(20251127)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "az.style.use('arviz-darkgrid')\n",
    "\n",
    "results_dirs = create_results_directory(base_dir='../results/03_basic_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ed217",
   "metadata": {},
   "source": [
    "# 1 DATA PREPARATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303c11c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split:\n",
      "  Train: 2020-11-25 to 2024-11-20\n",
      "  Test:  2024-11-21 to 2025-11-21\n",
      "  Train size: 1003 observations\n",
      "  Test size: 251 observations\n",
      "Total stocks: 7\n",
      "Train period: 2020-11-25 00:00:00 to 2024-11-20 00:00:00\n",
      "Test period: 2024-11-21 00:00:00 to 2025-11-21 00:00:00\n",
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "returns_df = pd.read_csv('../data/processed/log_returns.csv', index_col=0, parse_dates=True)\n",
    "train_df, test_df, split_date = train_test_split_timeseries(returns_df, train_ratio=0.8)\n",
    "stock_list = returns_df.columns.tolist()\n",
    "\n",
    "print(f\"Total stocks: {len(stock_list)}\")\n",
    "print(f\"Train period: {train_df.index[0]} to {train_df.index[-1]}\")\n",
    "print(f\"Test period: {test_df.index[0]} to {test_df.index[-1]}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "for idx, stock in enumerate(stock_list[:3]):\n",
    "    ax = axes[idx]\n",
    "    ax.plot(train_df.index, train_df[stock], 'b-', label='Train', alpha=0.7, linewidth=1)\n",
    "    ax.plot(test_df.index, test_df[stock], 'r-', label='Test', alpha=0.7, linewidth=1)\n",
    "    ax.axvline(split_date, color='black', linestyle='--', linewidth=2, alpha=0.5)\n",
    "    ax.set_title(f'{stock}: Train-Test Split')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Log Return')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dirs['figures']}/01_train_test_split.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3a209",
   "metadata": {},
   "source": [
    "# 2: AR(1) MODEL - FREQUENTIST ESTIMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9bbcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting AR(1) using MLE...\n",
      "\n",
      "AR(1) FREQUENTIST RESULTS:\n",
      "             mu       phi     sigma  log_likelihood          aic\n",
      "AAPL   0.000700  0.001282  0.016902     2666.716664 -5327.433329\n",
      "AMZN   0.000242  0.000464  0.022156     2395.497315 -4784.994630\n",
      "GOOGL  0.000696 -0.005942  0.019166     2540.770824 -5075.541648\n",
      "META   0.000726 -0.008070  0.028492     2143.477688 -4280.955375\n",
      "MSFT   0.000706 -0.016698  0.016395     2697.242830 -5388.485660\n",
      "NVDA   0.002476 -0.032073  0.032823     2001.676721 -3997.353441\n",
      "TSLA   0.000598 -0.029272  0.037587     1865.887725 -3725.775449\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitting AR(1) using MLE...\")\n",
    "\n",
    "ar1_freq_results = {}\n",
    "ar1_freq_params = {}\n",
    "\n",
    "for stock in stock_list:\n",
    "    returns_train = train_df[stock].values\n",
    "    model = AutoReg(returns_train, lags=1, seasonal=False, trend='c')\n",
    "    results = model.fit()\n",
    "    \n",
    "    ar1_freq_results[stock] = {\n",
    "        'model': model,\n",
    "        'results': results,\n",
    "        'returns_train': returns_train,\n",
    "        'returns_test': test_df[stock].values\n",
    "    }\n",
    "    \n",
    "    params = results.params\n",
    "    mu = params[0]\n",
    "    phi = params[1]\n",
    "    sigma_sq = results.resid.var()\n",
    "    \n",
    "    ar1_freq_params[stock] = {\n",
    "        'mu': mu,\n",
    "        'phi': phi,\n",
    "        'sigma_sq': sigma_sq,\n",
    "        'sigma': np.sqrt(sigma_sq),\n",
    "        'aic': results.aic,\n",
    "        'bic': results.bic,\n",
    "        'log_likelihood': results.llf\n",
    "    }\n",
    "\n",
    "ar1_freq_summary = pd.DataFrame(ar1_freq_params).T\n",
    "ar1_freq_summary = ar1_freq_summary[['mu', 'phi', 'sigma', 'log_likelihood', 'aic']]\n",
    "\n",
    "print(\"\\nAR(1) FREQUENTIST RESULTS:\")\n",
    "print(ar1_freq_summary.to_string())\n",
    "\n",
    "ar1_freq_summary.to_csv(f\"{results_dirs['ar1_freq']}/ar1_frequentist_summary.csv\")\n",
    "\n",
    "# Diagnostics\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for idx, stock in enumerate(stock_list[:2]):\n",
    "    results = ar1_freq_results[stock]['results']\n",
    "    resid = results.resid\n",
    "    \n",
    "    ax = axes[idx, 0]\n",
    "    ax.plot(resid, 'b-', alpha=0.7, linewidth=1)\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(f'{stock}: Residuals')\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[idx, 1]\n",
    "    sm.graphics.tsa.plot_acf(resid, lags=20, ax=ax)\n",
    "    ax.set_title(f'{stock}: ACF of Residuals')\n",
    "    \n",
    "    ax = axes[idx, 2]\n",
    "    ax.hist(resid, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(f'{stock}: Residual Distribution')\n",
    "    ax.set_xlabel('Residual')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "    ax = axes[idx, 3]\n",
    "    stats.probplot(resid, dist=\"norm\", plot=ax)\n",
    "    ax.set_title(f'{stock}: Q-Q Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dirs['ar1_freq']}/ar1_frequentist_diagnostics.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674dd807",
   "metadata": {},
   "source": [
    "# 3: AR(1) MODEL - BAYESIAN ESTIMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729ba49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling Bayesian AR(1) models...\n",
      "\n",
      "Sampling AAPL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, phi, sigma_sq]\n",
      "Sampling 2 chains for 500 tune and 1_000 draw iterations (1_000 + 2_000 draws total) took 24 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling AMZN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, phi, sigma_sq]\n",
      "Sampling 2 chains for 500 tune and 1_000 draw iterations (1_000 + 2_000 draws total) took 16 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling GOOGL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, phi, sigma_sq]\n",
      "Sampling 2 chains for 500 tune and 1_000 draw iterations (1_000 + 2_000 draws total) took 16 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling META...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, phi, sigma_sq]\n",
      "Sampling 2 chains for 500 tune and 1_000 draw iterations (1_000 + 2_000 draws total) took 16 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling MSFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, phi, sigma_sq]\n",
      "Sampling 2 chains for 500 tune and 1_000 draw iterations (1_000 + 2_000 draws total) took 16 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling NVDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, phi, sigma_sq]\n",
      "Sampling 2 chains for 500 tune and 1_000 draw iterations (1_000 + 2_000 draws total) took 16 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling TSLA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, phi, sigma_sq]\n",
      "Sampling 2 chains for 500 tune and 1_000 draw iterations (1_000 + 2_000 draws total) took 16 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AR(1) BAYESIAN POSTERIOR ESTIMATES:\n",
      "       mu_mean  mu_std  phi_mean  phi_std  sigma_mean\n",
      "AAPL     0.001   0.001     0.002    0.032    0.000000\n",
      "AMZN     0.000   0.001    -0.001    0.032    0.000000\n",
      "GOOGL    0.001   0.001    -0.004    0.032    0.000000\n",
      "META     0.001   0.001    -0.007    0.032    0.031623\n",
      "MSFT     0.001   0.001    -0.016    0.032    0.000000\n",
      "NVDA     0.002   0.001    -0.030    0.032    0.031623\n",
      "TSLA     0.001   0.001    -0.028    0.033    0.031623\n",
      "\n",
      "CONVERGENCE DIAGNOSTICS - MCMC QUALITY METRICS:\n",
      "Stock Parameter  R_hat  ESS_bulk  ESS_tail   Mean   Std\n",
      " AAPL        mu    1.0    1437.0    1475.0  0.001 0.001\n",
      " AAPL       phi    1.0    2122.0    1609.0  0.002 0.032\n",
      " AAPL  sigma_sq    1.0    2354.0    1444.0  0.000 0.000\n",
      " AMZN        mu    1.0    1837.0    1595.0  0.000 0.001\n",
      " AMZN       phi    1.0    1746.0    1198.0 -0.001 0.032\n",
      " AMZN  sigma_sq    1.0    1922.0    1392.0  0.000 0.000\n",
      "GOOGL        mu    1.0    2309.0    1656.0  0.001 0.001\n",
      "GOOGL       phi    1.0    1953.0    1386.0 -0.004 0.032\n",
      "GOOGL  sigma_sq    1.0    2210.0    1531.0  0.000 0.000\n",
      " META        mu    1.0    1973.0    1205.0  0.001 0.001\n",
      " META       phi    1.0    2306.0    1646.0 -0.007 0.032\n",
      " META  sigma_sq    1.0    2129.0    1348.0  0.001 0.000\n",
      " MSFT        mu    1.0    2129.0    1251.0  0.001 0.001\n",
      " MSFT       phi    1.0    2617.0    1856.0 -0.016 0.032\n",
      " MSFT  sigma_sq    1.0    2424.0    1556.0  0.000 0.000\n",
      " NVDA        mu    1.0    2461.0    1643.0  0.002 0.001\n",
      " NVDA       phi    1.0    2137.0    1397.0 -0.030 0.032\n",
      " NVDA  sigma_sq    1.0    2289.0    1484.0  0.001 0.000\n",
      " TSLA        mu    1.0    1733.0    1602.0  0.001 0.001\n",
      " TSLA       phi    1.0    2224.0    1530.0 -0.028 0.033\n",
      " TSLA  sigma_sq    1.0    2168.0    1460.0  0.001 0.000\n",
      "\n",
      "All parameters have converged (R_hat < 1.01).\n",
      "\n",
      "EFFECTIVE SAMPLE SIZE (ESS) ANALYSIS:\n",
      "\n",
      "Minimum ESS by parameter:\n",
      "  mu:      1437\n",
      "  phi:     1746\n",
      "  sigma_sq: 1922\n",
      "\n",
      "Generating MCMC trace plots...\n",
      "Trace plots saved.\n",
      "AR(1) Bayesian estimation complete.\n"
     ]
    }
   ],
   "source": [
    "def build_ar1_bayesian_model(returns):\n",
    "    \"\"\"\n",
    "    AR(1) Bayesian Model\n",
    "    Model: r_t = mu + phi * r_{t-1} + epsilon_t\n",
    "    where epsilon_t ~ N(0, sigma^2)\n",
    "    \"\"\"\n",
    "    prior_spec = AR1_priors.get_pymc_spec()\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # Priors\n",
    "        mu = pm.Normal('mu', \n",
    "                      mu=prior_spec['mu'][1]['mu'], \n",
    "                      sigma=prior_spec['mu'][1]['sigma'])\n",
    "        \n",
    "        phi = pm.Normal('phi', \n",
    "                       mu=prior_spec['phi'][1]['mu'], \n",
    "                       sigma=prior_spec['phi'][1]['sigma'])\n",
    "        \n",
    "        sigma_sq = pm.HalfCauchy('sigma_sq', \n",
    "                                beta=prior_spec['sigma_sq'][1]['beta'])\n",
    "        \n",
    "        # Likelihood\n",
    "        r_lag = returns[:-1]\n",
    "        r_curr = returns[1:]\n",
    "        mu_t = mu + phi * r_lag\n",
    "        \n",
    "        pm.Normal('likelihood', \n",
    "                 mu=mu_t, \n",
    "                 sigma=pm.math.sqrt(sigma_sq), \n",
    "                 observed=r_curr)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "prior_spec = AR1_priors.get_pymc_spec()\n",
    "ar1_bayes_results = {}\n",
    "print(\"\\nSampling Bayesian AR(1) models...\\n\")\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"Sampling {stock}...\")\n",
    "    returns_train = train_df[stock].values\n",
    "    model = build_ar1_bayesian_model(returns_train)\n",
    "    \n",
    "    with model:\n",
    "        idata = pm.sample(\n",
    "            draws=1000, \n",
    "            tune=500, \n",
    "            chains=2, \n",
    "            cores=4,           \n",
    "            random_seed=42, \n",
    "            return_inferencedata=True,\n",
    "            progressbar=False,\n",
    "            target_accept=0.8  \n",
    "        )\n",
    "    \n",
    "    ar1_bayes_results[stock] = {\n",
    "        'model': model,\n",
    "        'idata': idata,\n",
    "        'returns_train': returns_train,\n",
    "        'returns_test': test_df[stock].values\n",
    "    }\n",
    "\n",
    "# Extract results\n",
    "ar1_bayes_summary_table = {}\n",
    "\n",
    "for stock in stock_list:\n",
    "    idata = ar1_bayes_results[stock]['idata']\n",
    "    summary = az.summary(idata, var_names=['mu', 'phi', 'sigma_sq'])\n",
    "    \n",
    "    ar1_bayes_summary_table[stock] = {\n",
    "        'mu_mean': summary.loc['mu', 'mean'],\n",
    "        'mu_std': summary.loc['mu', 'sd'],\n",
    "        'phi_mean': summary.loc['phi', 'mean'],\n",
    "        'phi_std': summary.loc['phi', 'sd'],\n",
    "        'sigma_sq_mean': summary.loc['sigma_sq', 'mean'],\n",
    "        'sigma_sq_std': summary.loc['sigma_sq', 'sd'],\n",
    "        'sigma_mean': np.sqrt(summary.loc['sigma_sq', 'mean']),\n",
    "        'mu_hdi_low': summary.loc['mu', 'hdi_3%'],\n",
    "        'mu_hdi_high': summary.loc['mu', 'hdi_97%'],\n",
    "        'phi_hdi_low': summary.loc['phi', 'hdi_3%'],\n",
    "        'phi_hdi_high': summary.loc['phi', 'hdi_97%'],\n",
    "        'r_hat_mu': summary.loc['mu', 'r_hat'],\n",
    "        'r_hat_phi': summary.loc['phi', 'r_hat'],\n",
    "        'r_hat_sigma': summary.loc['sigma_sq', 'r_hat'],\n",
    "        'ess_bulk_mu': summary.loc['mu', 'ess_bulk'],\n",
    "        'ess_bulk_phi': summary.loc['phi', 'ess_bulk'],\n",
    "        'ess_bulk_sigma': summary.loc['sigma_sq', 'ess_bulk'],\n",
    "    }\n",
    "\n",
    "ar1_bayes_df = pd.DataFrame(ar1_bayes_summary_table).T\n",
    "\n",
    "print(\"\\nAR(1) BAYESIAN POSTERIOR ESTIMATES:\")\n",
    "print(ar1_bayes_df[[\n",
    "    'mu_mean', 'mu_std', 'phi_mean', 'phi_std', 'sigma_mean'\n",
    "]].round(6).to_string())\n",
    "\n",
    "\n",
    "print(\"\\nCONVERGENCE DIAGNOSTICS - MCMC QUALITY METRICS:\")\n",
    "diagnostics_data = []\n",
    "for stock in stock_list:\n",
    "    summary = az.summary(ar1_bayes_results[stock]['idata'], \n",
    "                        var_names=['mu', 'phi', 'sigma_sq'])\n",
    "    \n",
    "    for param in ['mu', 'phi', 'sigma_sq']:\n",
    "        diagnostics_data.append({\n",
    "            'Stock': stock,\n",
    "            'Parameter': param,\n",
    "            'R_hat': summary.loc[param, 'r_hat'],\n",
    "            'ESS_bulk': summary.loc[param, 'ess_bulk'],\n",
    "            'ESS_tail': summary.loc[param, 'ess_tail'],\n",
    "            'Mean': summary.loc[param, 'mean'],\n",
    "            'Std': summary.loc[param, 'sd']\n",
    "        })\n",
    "\n",
    "diagnostics_df = pd.DataFrame(diagnostics_data)\n",
    "print(diagnostics_df.to_string(index=False))\n",
    "\n",
    "convergence_ok = (diagnostics_df['R_hat'] < 1.01).all()\n",
    "if convergence_ok:\n",
    "    print(\"\\nAll parameters have converged (R_hat < 1.01).\")\n",
    "else:\n",
    "    print(\"\\nWARNING - Some parameters have not converged (R_hat >= 1.01).\")\n",
    "\n",
    "diagnostics_df.to_csv(f\"{results_dirs['ar1_bayes']}/ar1_mcmc_diagnostics.csv\", index=False)\n",
    "\n",
    "print(\"\\nEFFECTIVE SAMPLE SIZE (ESS) ANALYSIS:\")\n",
    "print(\"\\nMinimum ESS by parameter:\")\n",
    "print(f\"  mu:      {diagnostics_df[diagnostics_df['Parameter']=='mu']['ESS_bulk'].min():.0f}\")\n",
    "print(f\"  phi:     {diagnostics_df[diagnostics_df['Parameter']=='phi']['ESS_bulk'].min():.0f}\")\n",
    "print(f\"  sigma_sq: {diagnostics_df[diagnostics_df['Parameter']=='sigma_sq']['ESS_bulk'].min():.0f}\")\n",
    "\n",
    "\n",
    "print(\"\\nGenerating MCMC trace plots...\")\n",
    "for stock in stock_list[:2]:\n",
    "    idata = ar1_bayes_results[stock]['idata']\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n",
    "    \n",
    "    for idx, param in enumerate(['mu', 'phi', 'sigma_sq']):\n",
    "        # Trace\n",
    "        ax = axes[idx, 0]\n",
    "        chains = idata.posterior[param].values\n",
    "        for chain in chains:\n",
    "            ax.plot(chain, alpha=0.7, linewidth=0.8)\n",
    "        ax.set_title(f'{stock}: {param} - Trace')\n",
    "        ax.set_ylabel(param)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Posterior\n",
    "        ax = axes[idx, 1]\n",
    "        posterior = chains.flatten()\n",
    "        ax.hist(posterior, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(posterior.mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "        ax.set_title(f'{stock}: {param} - Posterior')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{results_dirs['ar1_bayes']}/ar1_trace_posterior_{stock}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(\"Trace plots saved.\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "for idx, stock in enumerate(stock_list[:2]):\n",
    "    idata = ar1_bayes_results[stock]['idata']\n",
    "    \n",
    "    # Posterior mu\n",
    "    ax = axes[idx, 0]\n",
    "    posterior_mu = idata.posterior['mu'].values.flatten()\n",
    "    ax.hist(posterior_mu, bins=40, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax.axvline(posterior_mu.mean(), color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_title(f'{stock}: Posterior of mu')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Posterior phi\n",
    "    ax = axes[idx, 1]\n",
    "    posterior_phi = idata.posterior['phi'].values.flatten()\n",
    "    ax.hist(posterior_phi, bins=40, density=True, alpha=0.7, color='forestgreen', edgecolor='black')\n",
    "    ax.axvline(posterior_phi.mean(), color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_title(f'{stock}: Posterior of phi')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dirs['ar1_bayes']}/ar1_posterior_distributions.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save results\n",
    "ar1_bayes_df.to_csv(f\"{results_dirs['ar1_bayes']}/ar1_bayesian_summary.csv\")\n",
    "\n",
    "print(\"AR(1) Bayesian estimation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1395e",
   "metadata": {},
   "source": [
    "# 4: GARCH(1,1) - FREQUENTIST ESTIMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb12654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting GARCH(1,1) models using Maximum Likelihood Estimation...\n",
      "\n",
      "Fitting AAPL...\n",
      "  mu=0.000969, omega=0.000004, alpha=0.0343, beta=0.9525\n",
      "Fitting AMZN...\n",
      "  mu=0.000653, omega=0.000049, alpha=0.2000, beta=0.7000\n",
      "Fitting GOOGL...\n",
      "  mu=0.000890, omega=0.000007, alpha=0.0100, beta=0.9700\n",
      "Fitting META...\n",
      "  mu=0.001057, omega=0.000016, alpha=0.0100, beta=0.9700\n",
      "Fitting MSFT...\n",
      "  mu=0.000947, omega=0.000002, alpha=0.0275, beta=0.9656\n",
      "Fitting NVDA...\n",
      "  mu=0.002785, omega=0.000022, alpha=0.0100, beta=0.9700\n",
      "Fitting TSLA...\n",
      "  mu=0.000963, omega=0.000034, alpha=0.0338, beta=0.9434\n",
      "\n",
      "GARCH(1,1) FREQUENTIST RESULTS:\n",
      "             mu     omega     alpha      beta  alpha_plus_beta\n",
      "AAPL   0.000969  0.000004  0.034318  0.952460         0.986778\n",
      "AMZN   0.000653  0.000049  0.200000  0.700000         0.900000\n",
      "GOOGL  0.000890  0.000007  0.010000  0.970000         0.980000\n",
      "META   0.001057  0.000016  0.010024  0.969977         0.980001\n",
      "MSFT   0.000947  0.000002  0.027469  0.965572         0.993041\n",
      "NVDA   0.002785  0.000022  0.010001  0.969999         0.980000\n",
      "TSLA   0.000963  0.000034  0.033751  0.943389         0.977140\n"
     ]
    }
   ],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "garch_freq_results = {}\n",
    "garch_freq_params = {}\n",
    "\n",
    "print(\"\\nFitting GARCH(1,1) models using Maximum Likelihood Estimation...\\n\")\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"Fitting {stock}...\")\n",
    "    \n",
    "    returns_train = train_df[stock].dropna().values\n",
    "    \n",
    "    model = arch_model(\n",
    "        returns_train, \n",
    "        mean='Constant', \n",
    "        vol='Garch', \n",
    "        p=1, q=1,  \n",
    "        rescale=False  \n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        fit_res = model.fit(disp='off', show_warning=False)\n",
    "        \n",
    "        params = fit_res.params\n",
    "        mu = params['mu']\n",
    "        omega = params['omega']\n",
    "        alpha = params['alpha[1]'] \n",
    "        beta = params['beta[1]']  \n",
    "        \n",
    "        long_run_var = omega / (1 - alpha - beta) if (alpha + beta) < 0.999 else np.nan\n",
    "        \n",
    "        garch_freq_results[stock] = {\n",
    "            'fit': fit_res,\n",
    "            'nobs': len(returns_train)\n",
    "        }\n",
    "        \n",
    "        garch_freq_params[stock] = {\n",
    "            'mu': mu,\n",
    "            'omega': omega,\n",
    "            'alpha': alpha,\n",
    "            'beta': beta,\n",
    "            'alpha_plus_beta': alpha + beta,\n",
    "            'long_run_var': long_run_var,\n",
    "            'aic': fit_res.aic,\n",
    "            'bic': fit_res.bic,\n",
    "            'log_likelihood': fit_res.loglikelihood\n",
    "        }\n",
    "        \n",
    "        print(f\"  mu={mu:.6f}, omega={omega:.6f}, alpha={alpha:.4f}, beta={beta:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error fitting GARCH for {stock}: {e}\")\n",
    "        garch_freq_params[stock] = {'error': str(e)}\n",
    "\n",
    "garch_freq_summary = pd.DataFrame(garch_freq_params).T\n",
    "garch_freq_display = garch_freq_summary[['mu', 'omega', 'alpha', 'beta', 'alpha_plus_beta']]\n",
    "\n",
    "print(\"\\nGARCH(1,1) FREQUENTIST RESULTS:\")\n",
    "print(garch_freq_display.round(6).to_string())\n",
    "garch_freq_summary.to_csv(f\"{results_dirs['garch_bayes']}/garch_freq_params.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4472b5f",
   "metadata": {},
   "source": [
    "# 5: GARCH(1,1) - BAYESIAN ESTIMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43532e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling Bayesian GARCH(1,1) for AAPL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, omega, alpha, beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44370d14aaec47d2b536177a2ce8e97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 400 tune and 750 draw iterations (800 + 1_500 draws total) took 250 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged: R_hat=1.010, ESS=385\n",
      "  Params: μ=0.001000, α=0.080, β=0.853, α+β=0.933 ✓\n",
      "\n",
      "Sampling Bayesian GARCH(1,1) for AMZN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, omega, alpha, beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0d0564ea844f9d9433026aaa9f929e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 400 tune and 750 draw iterations (800 + 1_500 draws total) took 234 seconds.\n",
      "There were 34 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged: R_hat=1.000, ESS=465\n",
      "  Params: μ=0.001000, α=0.190, β=0.738, α+β=0.928 ✓\n",
      "\n",
      "Sampling Bayesian GARCH(1,1) for GOOGL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, omega, alpha, beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc50eb5fd574502aab5b74b7eefb8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 400 tune and 750 draw iterations (800 + 1_500 draws total) took 307 seconds.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged: R_hat=1.000, ESS=360\n",
      "  Params: μ=0.001000, α=0.076, β=0.791, α+β=0.867 ✓\n",
      "\n",
      "Sampling Bayesian GARCH(1,1) for META...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, omega, alpha, beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c4355a80b24625ba41dbc29a83c6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 400 tune and 750 draw iterations (800 + 1_500 draws total) took 239 seconds.\n",
      "There were 188 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged: R_hat=1.000, ESS=502\n",
      "  Params: μ=0.001000, α=0.233, β=0.684, α+β=0.917 ✓\n",
      "\n",
      "Sampling Bayesian GARCH(1,1) for MSFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, omega, alpha, beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87c5164f29742eeae447f26967bba36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 400 tune and 750 draw iterations (800 + 1_500 draws total) took 257 seconds.\n",
      "There were 10 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged: R_hat=1.000, ESS=441\n",
      "  Params: μ=0.001000, α=0.047, β=0.933, α+β=0.980 ✓\n",
      "\n",
      "Sampling Bayesian GARCH(1,1) for NVDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, omega, alpha, beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5338cc5e16884413a208eb3441af99d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 400 tune and 750 draw iterations (800 + 1_500 draws total) took 298 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged: R_hat=1.010, ESS=419\n",
      "  Params: μ=0.003000, α=0.051, β=0.904, α+β=0.955 ✓\n",
      "\n",
      "Sampling Bayesian GARCH(1,1) for TSLA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, omega, alpha, beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b113bce7f64178ab219b92a6848acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 400 tune and 750 draw iterations (800 + 1_500 draws total) took 322 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged: R_hat=1.000, ESS=799\n",
      "  Params: μ=0.001000, α=0.055, β=0.863, α+β=0.918 ✓\n",
      "BAYESIAN GARCH(1,1) SUMMARY (CORRECT TIME-VARYING VOLATILITY)\n",
      "      mu_mean omega_mean alpha_mean beta_mean alpha_plus_beta stationary\n",
      "AAPL    0.001        0.0       0.08     0.853           0.933       True\n",
      "AMZN    0.001        0.0       0.19     0.738           0.928       True\n",
      "GOOGL   0.001        0.0      0.076     0.791           0.867       True\n",
      "META    0.001        0.0      0.233     0.684           0.917       True\n",
      "MSFT    0.001        0.0      0.047     0.933            0.98       True\n",
      "NVDA    0.003        0.0      0.051     0.904           0.955       True\n",
      "TSLA    0.001        0.0      0.055     0.863           0.918       True\n"
     ]
    }
   ],
   "source": [
    "def build_garch_bayesian_model(returns):\n",
    "    \"\"\"\n",
    "    GARCH(1,1) Bayesian Model with time-varying volatility\n",
    "    r_t = mu + epsilon_t, epsilon_t ~ N(0, sqrt(h_t))\n",
    "    h_t = omega + alpha * epsilon_{t-1}^2 + beta * h_{t-1}\n",
    "    \"\"\"\n",
    "    prior_spec = GARCH_priors.get_pymc_spec()\n",
    "    \n",
    "    n_obs = len(returns)\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        mu = pm.Normal('mu', **prior_spec['mu'][1])\n",
    "        omega = pm.HalfNormal('omega', **prior_spec['omega'][1])\n",
    "        alpha = pm.Beta('alpha', **prior_spec['alpha'][1])\n",
    "        beta = pm.Beta('beta', **prior_spec['beta'][1])\n",
    "        \n",
    "        # Add the stationarity constraint (CRITICAL for GARCH)\n",
    "        pm.Potential('stationarity_potential', \n",
    "                     pm.math.switch(alpha + beta < 1, 0, -np.inf))\n",
    "        \n",
    "        # Fixed h_init (pragmatic for speed)\n",
    "        h_init = pt.as_tensor_variable(np.var(returns) * 0.1)\n",
    "        \n",
    "        residuals = returns - mu\n",
    "        resid_sq = pt.square(residuals)\n",
    "\n",
    "        # garch_step function for scan\n",
    "        def garch_step(resid_t_sq, h_tm1, omega_, alpha_, beta_):\n",
    "            h_t = omega_ + alpha_ * resid_t_sq + beta_ * h_tm1\n",
    "            return h_t\n",
    "        \n",
    "        h_sequence, _ = scan(\n",
    "            fn=garch_step,\n",
    "            sequences=resid_sq[:-1],\n",
    "            outputs_info=h_init,\n",
    "            non_sequences=[omega, alpha, beta],\n",
    "            n_steps=n_obs-1\n",
    "        )\n",
    "        \n",
    "        h_full = pt.concatenate([[h_init], h_sequence])\n",
    "        h_pos = pt.maximum(h_full, 1e-8) \n",
    "        \n",
    "        pm.Normal('likelihood', mu=mu, sigma=pt.sqrt(h_pos), observed=returns)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "garch_bayes_results = {}\n",
    "garch_bayes_summary_table = {}\n",
    "\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"\\nSampling Bayesian GARCH(1,1) for {stock}...\")\n",
    "    \n",
    "    returns_train = train_df[stock].dropna().values\n",
    "    \n",
    "\n",
    "    model = build_garch_bayesian_model(returns_train)\n",
    "    \n",
    "    with model:\n",
    "        idata = pm.sample(\n",
    "            draws=750,            \n",
    "            tune=400,             \n",
    "            chains=2,\n",
    "            cores=2,\n",
    "            random_seed=42,\n",
    "            target_accept=0.80,  \n",
    "            return_inferencedata=True,\n",
    "            progressbar=True,\n",
    "            nuts_sampler_kwargs={'max_treedepth': 10} \n",
    "        )\n",
    "    \n",
    "    summary = az.summary(idata, var_names=['mu', 'omega', 'alpha', 'beta'])\n",
    "    \n",
    "    alpha_plus_beta = summary.loc['alpha', 'mean'] + summary.loc['beta', 'mean']\n",
    "    stationarity_ok = alpha_plus_beta < 0.999\n",
    "    \n",
    "    garch_bayes_results[stock] = {'idata': idata, 'model': model}\n",
    "    \n",
    "    garch_bayes_summary_table[stock] = {\n",
    "        'mu_mean': summary.loc['mu', 'mean'],\n",
    "        'mu_hdi': f\"[{summary.loc['mu', 'hdi_3%']:.4f}, {summary.loc['mu', 'hdi_97%']:.4f}]\",\n",
    "        'omega_mean': summary.loc['omega', 'mean'],\n",
    "        'alpha_mean': summary.loc['alpha', 'mean'],\n",
    "        'beta_mean': summary.loc['beta', 'mean'],\n",
    "        'alpha_plus_beta': alpha_plus_beta,\n",
    "        'stationary': stationarity_ok,\n",
    "        'r_hat_max': summary['r_hat'].max(),\n",
    "        'ess_bulk_min': summary['ess_bulk'].min(),\n",
    "        'mu_std': summary.loc['mu', 'sd'],\n",
    "        'omega_std': summary.loc['omega', 'sd'],\n",
    "        'alpha_std': summary.loc['alpha', 'sd'],\n",
    "        'beta_std': summary.loc['beta', 'sd']\n",
    "    }\n",
    "    \n",
    "    print(f\"  Converged: R_hat={summary['r_hat'].max():.3f}, ESS={summary['ess_bulk'].min():.0f}\")\n",
    "    print(f\"  Params: μ={summary.loc['mu', 'mean']:.6f}, \"\n",
    "            f\"α={summary.loc['alpha', 'mean']:.3f}, β={summary.loc['beta', 'mean']:.3f}, \"\n",
    "            f\"α+β={alpha_plus_beta:.3f} {'✓' if stationarity_ok else '✗'}\")\n",
    "              \n",
    "\n",
    "garch_bayes_df = pd.DataFrame(garch_bayes_summary_table).T\n",
    "print(\"BAYESIAN GARCH(1,1) SUMMARY (CORRECT TIME-VARYING VOLATILITY)\")\n",
    "display_cols = ['mu_mean', 'omega_mean', 'alpha_mean', 'beta_mean', 'alpha_plus_beta', 'stationary']\n",
    "print(garch_bayes_df[display_cols].round(6))\n",
    "\n",
    "garch_bayes_df.to_csv(f\"{results_dirs['garch_bayes']}/garch_bayesian_correct.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbaea35",
   "metadata": {},
   "source": [
    "# 6: FREQUENTIST vs BAYESIAN COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4fb6a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AR(1) MODEL COMPARISON:\n",
      "        Freq_mu  Bayes_mu  Freq_phi  Bayes_phi  Freq_sigma  Bayes_sigma\n",
      "Stock                                                                  \n",
      "AAPL   0.000700     0.001  0.001282      0.002    0.016902     0.000000\n",
      "AMZN   0.000242     0.000  0.000464     -0.001    0.022156     0.000000\n",
      "GOOGL  0.000696     0.001 -0.005942     -0.004    0.019166     0.000000\n",
      "META   0.000726     0.001 -0.008070     -0.007    0.028492     0.031623\n",
      "MSFT   0.000706     0.001 -0.016698     -0.016    0.016395     0.000000\n",
      "NVDA   0.002476     0.002 -0.032073     -0.030    0.032823     0.031623\n",
      "TSLA   0.000598     0.001 -0.029272     -0.028    0.037587     0.031623\n",
      "\n",
      "GARCH(1,1) MODEL COMPARISON:\n",
      "        Freq_mu  Bayes_mu  Freq_alpha  Bayes_alpha  Freq_beta  Bayes_beta\n",
      "Stock                                                                    \n",
      "AAPL   0.000969     0.001    0.034318        0.080   0.952460       0.853\n",
      "AMZN   0.000653     0.001    0.200000        0.190   0.700000       0.738\n",
      "GOOGL  0.000890     0.001    0.010000        0.076   0.970000       0.791\n",
      "META   0.001057     0.001    0.010024        0.233   0.969977       0.684\n",
      "MSFT   0.000947     0.001    0.027469        0.047   0.965572       0.933\n",
      "NVDA   0.002785     0.003    0.010001        0.051   0.969999       0.904\n",
      "TSLA   0.000963     0.001    0.033751        0.055   0.943389       0.863\n"
     ]
    }
   ],
   "source": [
    "# AR(1) Comparison\n",
    "print(\"\\nAR(1) MODEL COMPARISON:\")\n",
    "\n",
    "ar1_comparison = []\n",
    "for stock in stock_list:\n",
    "    ar1_comparison.append({\n",
    "        'Stock': stock,\n",
    "        'Freq_mu': ar1_freq_params[stock]['mu'],\n",
    "        'Bayes_mu': ar1_bayes_df.loc[stock, 'mu_mean'],\n",
    "        'Freq_phi': ar1_freq_params[stock]['phi'],\n",
    "        'Bayes_phi': ar1_bayes_df.loc[stock, 'phi_mean'],\n",
    "        'Freq_sigma': ar1_freq_params[stock]['sigma'],\n",
    "        'Bayes_sigma': ar1_bayes_df.loc[stock, 'sigma_mean'],\n",
    "    })\n",
    "\n",
    "ar1_comp_df = pd.DataFrame(ar1_comparison).set_index('Stock')\n",
    "print(ar1_comp_df.round(6).to_string())\n",
    "ar1_comp_df.to_csv(f\"{results_dirs['ar1_bayes']}/ar1_comparison.csv\")\n",
    "\n",
    "# GARCH(1,1) Comparison\n",
    "print(\"\\nGARCH(1,1) MODEL COMPARISON:\")\n",
    "\n",
    "garch_comparison = []\n",
    "for stock in stock_list:\n",
    "    if stock in garch_freq_params and stock in garch_bayes_df.index:\n",
    "        if 'error' not in str(garch_bayes_df.loc[stock].values):\n",
    "            garch_comparison.append({\n",
    "                'Stock': stock,\n",
    "                'Freq_mu': garch_freq_params[stock]['mu'],\n",
    "                'Bayes_mu': garch_bayes_df.loc[stock, 'mu_mean'],\n",
    "                'Freq_alpha': garch_freq_params[stock]['alpha'],\n",
    "                'Bayes_alpha': garch_bayes_df.loc[stock, 'alpha_mean'],\n",
    "                'Freq_beta': garch_freq_params[stock]['beta'],\n",
    "                'Bayes_beta': garch_bayes_df.loc[stock, 'beta_mean'],\n",
    "            })\n",
    "\n",
    "garch_comp_df = pd.DataFrame(garch_comparison).set_index('Stock')\n",
    "print(garch_comp_df.round(6).to_string())\n",
    "garch_comp_df.to_csv(f\"{results_dirs['garch_bayes']}/garch_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d97b4b",
   "metadata": {},
   "source": [
    "# 7: SV Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f12628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "FAST SV with Non-Centered Parameterization (5-10 min per stock)\n",
      "====================================================================================================\n",
      "\n",
      "AAPL... (n=1003) | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, phi_raw, alpha, sigma_eta, z]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     85\u001b[39m model = build_sv_bayesian_model_fast(returns_train)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m model:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     idata = \u001b[43mpm\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdraws\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchains\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcores\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_treedepth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogressbar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnuts_sampler_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstore_unconstrained_samples\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m summary = az.summary(idata, var_names=[\u001b[33m'\u001b[39m\u001b[33mmu\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mphi\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msigma_eta\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    103\u001b[39m sv_results[stock] = {\u001b[33m'\u001b[39m\u001b[33midata\u001b[39m\u001b[33m'\u001b[39m: idata, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: model}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\natha\\anaconda3\\envs\\bayesian_final\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:957\u001b[39m, in \u001b[36msample\u001b[39m\u001b[34m(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, compile_kwargs, **kwargs)\u001b[39m\n\u001b[32m    953\u001b[39m t_sampling = time.time() - t_start\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# Packaging, validating and returning the result was extracted\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;66;03m# into a function to make it easier to test and refactor.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZarrTrace\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\natha\\anaconda3\\envs\\bayesian_final\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:1042\u001b[39m, in \u001b[36m_sample_return\u001b[39m\u001b[34m(run, traces, tune, t_sampling, discard_tuned_samples, compute_convergence_checks, return_inferencedata, keep_warning_stat, idata_kwargs, model)\u001b[39m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# Pick and slice chains to keep the maximum number of samples\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m discard_tuned_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1042\u001b[39m     traces, length = \u001b[43m_choose_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1044\u001b[39m     traces, length = _choose_chains(traces, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\natha\\anaconda3\\envs\\bayesian_final\\Lib\\site-packages\\pymc\\backends\\base.py:624\u001b[39m, in \u001b[36m_choose_chains\u001b[39m\u001b[34m(traces, tune)\u001b[39m\n\u001b[32m    622\u001b[39m lengths = [\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trace) - tune) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces]\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths):\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot enough samples to build a trace.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    626\u001b[39m idxs = np.argsort(lengths)\n\u001b[32m    627\u001b[39m l_sort = np.array(lengths)[idxs]\n",
      "\u001b[31mValueError\u001b[39m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "from pymc import math as pmm\n",
    "\n",
    "def build_sv_bayesian_model_fast(returns, error_dist='normal'):\n",
    "    \"\"\"\n",
    "    r_t = μ + exp(h_t/2) * ε_t\n",
    "    h_t = α + φ * h_{t-1} + η_t\n",
    "    \n",
    "    REVISED: Priors adjusted to match typical financial data scale.\n",
    "    \"\"\"\n",
    "    n_obs = len(returns)\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        r = pm.Data(\"r\", returns)\n",
    "        T = r.shape[0]\n",
    "        mu = pm.Normal('mu', mu=0., sigma=0.01)\n",
    "\n",
    "        phi_raw = pm.Normal('phi_raw', mu=0., sigma=1.)\n",
    "        phi = pm.Deterministic('phi', pmm.tanh(phi_raw))\n",
    "\n",
    "    \n",
    "        alpha = pm.Normal('alpha', mu=-8., sigma=1.0) \n",
    "\n",
    "\n",
    "        sigma_eta = pm.HalfNormal('sigma_eta', sigma=0.1) \n",
    "\n",
    "        z = pm.Normal('z', mu=0., sigma=1., shape=T)\n",
    "\n",
    "        def ar1_step(z_t, h_prev, alpha_, phi_, sigma_eta_):\n",
    "            h_t = alpha_ + phi_ * (h_prev - alpha_) + sigma_eta_ * z_t\n",
    "            return h_t\n",
    "\n",
    "        h_seq, _ = scan( \n",
    "            fn=ar1_step,\n",
    "            sequences=[z],                       \n",
    "            outputs_info=[alpha],               \n",
    "            non_sequences=[alpha, phi, sigma_eta],\n",
    "            n_steps=T\n",
    "        )\n",
    "\n",
    "        h_clip = pmm.clip(h_seq, -30, 30)\n",
    "\n",
    "        # likelihood\n",
    "        if error_dist == 'normal':\n",
    "            pm.Normal('obs', mu=mu, sigma=pmm.exp(h_clip / 2), observed=r)\n",
    "        elif error_dist == 'studentt':\n",
    "            nu = pm.Exponential('nu', 1/30) \n",
    "            pm.StudentT('obs', nu=nu, mu=mu, sigma=pmm.exp(h_clip / 2), observed=r)\n",
    "        else:\n",
    "            raise ValueError(\"error_dist must be 'normal' or 'studentt'\")\n",
    "\n",
    "        pm.Deterministic('h_log_volatility_process', h_seq)\n",
    "\n",
    "    return model\n",
    "\n",
    "sv_results = {}\n",
    "sv_summary_table = {}\n",
    "\n",
    "print(\"SV with Non-Centered Parameterization\")\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"\\n{stock}...\", end=\" \", flush=True)\n",
    "    \n",
    "    returns_train = train_df[stock].dropna().values\n",
    "    n_obs = len(returns_train)\n",
    "    print(f\"(n={n_obs})\", end=\" | \")\n",
    "\n",
    "    model = build_sv_bayesian_model_fast(returns_train)\n",
    "    \n",
    "    with model:\n",
    "        idata = pm.sample(\n",
    "            draws=500,         \n",
    "            tune=500,          \n",
    "            chains=2,           \n",
    "            cores=2,            \n",
    "            random_seed=42,\n",
    "            target_accept=0.8, \n",
    "            max_treedepth=10,   \n",
    "            return_inferencedata=True,\n",
    "            progressbar=False,\n",
    "            nuts_sampler_kwargs={'store_unconstrained_samples': False}\n",
    "        )\n",
    "    \n",
    "    summary = az.summary(idata, var_names=['mu', 'alpha', 'phi', 'sigma_eta'])\n",
    "    \n",
    "    sv_results[stock] = {'idata': idata, 'model': model}\n",
    "    \n",
    "    mu_mean = summary.loc['mu', 'mean']\n",
    "    alpha_mean = summary.loc['alpha', 'mean']\n",
    "    phi_mean = summary.loc['phi', 'mean']\n",
    "    sigma_eta_mean = summary.loc['sigma_eta', 'mean']\n",
    "    vol_level = np.exp(alpha_mean / 2) * 100\n",
    "    rhat_max = summary['r_hat'].max()\n",
    "    ess_min = summary['ess_bulk'].min()\n",
    "    div_count = idata.sample_stats.divergent.sum().item()\n",
    "    \n",
    "    sv_summary_table[stock] = {\n",
    "        'mu': mu_mean,\n",
    "        'alpha': alpha_mean,\n",
    "        'phi': phi_mean,\n",
    "        'sigma_eta': sigma_eta_mean,\n",
    "        'vol_level_pct': vol_level,\n",
    "        'rhat_max': rhat_max,\n",
    "        'ess_bulk_min': ess_min,\n",
    "        'divergences': div_count\n",
    "    }\n",
    "    \n",
    "    status = \"v\" if (rhat_max < 1.05 and ess_min > 200 and div_count == 0) else \"x\"\n",
    "    print(f\"{status} φ={phi_mean:.3f}, σ_η={sigma_eta_mean:.3f}, Vol={vol_level:.1f}%\")\n",
    "    print(f\"   R̂={rhat_max:.3f}, ESS={ess_min:.0f}, Div={div_count}\")\n",
    "        \n",
    "print(\"SUMMARY\")\n",
    "\n",
    "sv_df = pd.DataFrame(sv_summary_table).T\n",
    "\n",
    "if 'error' not in sv_df.columns:\n",
    "    display_cols = ['phi', 'sigma_eta', 'vol_level_pct', 'rhat_max', 'ess_bulk_min', 'divergences']\n",
    "    print(sv_df[display_cols].round(3).to_string())\n",
    "else:\n",
    "    print(sv_df.to_string())\n",
    "\n",
    "sv_df.to_csv(f\"{results_dirs['sv']}/sv_fast_results.csv\")\n",
    "print(f\"\\nResults saved to {results_dirs['sv']}/sv_fast_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52831dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "FAST & ACCURATE STOCHASTIC VOLATILITY MODEL\n",
      "====================================================================================================\n",
      "\n",
      "[1/7] AAPL... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39065d2073b14c19b608bb239ff248f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, phi_raw, alpha, sigma_eta, z]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ ERROR: Not enough samples to build a trace.\n",
      "[2/7] AMZN... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_3240\\1922139345.py\", line 160, in <module>\n",
      "    model, idata = fit_sv_with_map_init(returns_train, stock, verbose=False)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_3240\\1922139345.py\", line 113, in fit_sv_with_map_init\n",
      "    idata = pm.sample(\n",
      "            ^^^^^^^^^^\n",
      "  File \"c:\\Users\\natha\\anaconda3\\envs\\bayesian_final\\Lib\\site-packages\\pymc\\sampling\\mcmc.py\", line 957, in sample\n",
      "    return _sample_return(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\natha\\anaconda3\\envs\\bayesian_final\\Lib\\site-packages\\pymc\\sampling\\mcmc.py\", line 1042, in _sample_return\n",
      "    traces, length = _choose_chains(traces, tune)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\natha\\anaconda3\\envs\\bayesian_final\\Lib\\site-packages\\pymc\\backends\\base.py\", line 624, in _choose_chains\n",
      "    raise ValueError(\"Not enough samples to build a trace.\")\n",
      "ValueError: Not enough samples to build a trace.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844993bc414047b692b0f8a468519ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu, phi_raw, alpha, sigma_eta, z]\n"
     ]
    }
   ],
   "source": [
    "from pymc import math as pmm\n",
    "from pytensor.scan import scan\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def build_sv_bayesian_model_fast(returns, error_dist='normal'):\n",
    "    \"\"\"\n",
    "    FAST & ACCURATE SV Model\n",
    "    \n",
    "    r_t = μ + exp(h_t/2) * ε_t\n",
    "    h_t = α + φ * h_{t-1} + η_t\n",
    "    \n",
    "    Key improvements:\n",
    "    - Adjusted priors for financial data\n",
    "    - Non-centered parameterization\n",
    "    - Proper initialization via MAP\n",
    "    - Numerical stability\n",
    "    \"\"\"\n",
    "    n_obs = len(returns)\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        r = pm.Data(\"r\", returns)\n",
    "        T = r.shape[0]\n",
    "        \n",
    "        # ✅ OBSERVATION MEAN: tight prior (financial returns ~0)\n",
    "        mu = pm.Normal('mu', mu=0., sigma=0.01)\n",
    "\n",
    "        # ✅ PERSISTENCE (phi): Beta reparameterization for (0, 1) support\n",
    "        # phi ~ Beta(20, 1.5) gives mode ~0.93, concentrated on (0.8, 1.0)\n",
    "        phi_raw = pm.Beta('phi_raw', alpha=20, beta=1.5)\n",
    "        phi = pm.Deterministic('phi', 0.9 + 0.099 * phi_raw)  # Rescale to [0.9, 0.999]\n",
    "\n",
    "        # ✅ INTERCEPT: centered at log(variance) ~ -8 to -4\n",
    "        alpha = pm.Normal('alpha', mu=-6.0, sigma=1.5)\n",
    "\n",
    "        # ✅ VOLATILITY OF VOLATILITY: HalfNormal is good\n",
    "        sigma_eta = pm.HalfNormal('sigma_eta', sigma=0.2)\n",
    "\n",
    "        # ✅ NON-CENTERED LATENT INNOVATIONS\n",
    "        z = pm.Normal('z', mu=0., sigma=1., shape=T)\n",
    "\n",
    "        # ✅ AR(1) RECURSION FUNCTION\n",
    "        def ar1_step(z_t, h_prev, alpha_, phi_, sigma_eta_):\n",
    "            \"\"\"\n",
    "            Centered parameterization:\n",
    "            h_t = α + φ*(h_{t-1} - α) + σ_η*z_t\n",
    "            \"\"\"\n",
    "            h_t = alpha_ + phi_ * (h_prev - alpha_) + sigma_eta_ * z_t\n",
    "            return h_t\n",
    "\n",
    "        # ✅ INITIAL STATE: stationary distribution mean\n",
    "        h0 = alpha\n",
    "\n",
    "        # ✅ SCAN: Build latent volatility sequence\n",
    "        h_seq, _ = scan(\n",
    "            fn=ar1_step,\n",
    "            sequences=[z],\n",
    "            outputs_info=[h0],\n",
    "            non_sequences=[alpha, phi, sigma_eta],\n",
    "            n_steps=T\n",
    "        )\n",
    "\n",
    "        # ✅ NUMERICAL STABILITY: clip to [-30, 20]\n",
    "        h_clip = pmm.clip(h_seq, -30, 20)\n",
    "\n",
    "        # ✅ OBSERVATION LIKELIHOOD\n",
    "        if error_dist == 'normal':\n",
    "            pm.Normal('obs', mu=mu, sigma=pmm.exp(h_clip / 2), observed=r)\n",
    "        elif error_dist == 'studentt':\n",
    "            nu = pm.Exponential('nu', 1/30)\n",
    "            pm.StudentT('obs', nu=nu, mu=mu, sigma=pmm.exp(h_clip / 2), observed=r)\n",
    "        else:\n",
    "            raise ValueError(\"error_dist must be 'normal' or 'studentt'\")\n",
    "\n",
    "        # ✅ EXPOSE LATENT VOLATILITY FOR DIAGNOSTICS\n",
    "        pm.Deterministic('h_seq', h_seq)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_sv_with_map_init(returns, stock_name='', verbose=True):\n",
    "    \"\"\"\n",
    "    Fit SV model with MAP initialization for speed & stability\n",
    "    \"\"\"\n",
    "    n_obs = len(returns)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Building model (n={n_obs})...\", end=\" \", flush=True)\n",
    "    \n",
    "    model = build_sv_bayesian_model_fast(returns)\n",
    "    \n",
    "    with model:\n",
    "        # ✅ STEP 1: Fast MAP estimation (30-60 sec)\n",
    "        if verbose:\n",
    "            print(\"MAP...\", end=\" \", flush=True)\n",
    "        \n",
    "        try:\n",
    "            map_est = pm.find_MAP(maxeval=3000, include_transformed=True)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"MAP failed ({str(e)[:30]}), using random init...\", end=\" \", flush=True)\n",
    "            map_est = None\n",
    "        \n",
    "        # ✅ STEP 2: NUTS Sampling (2-5 min)\n",
    "        if verbose:\n",
    "            print(\"NUTS...\", end=\" \", flush=True)\n",
    "        \n",
    "        idata = pm.sample(\n",
    "            draws=800,          # More draws for better ESS\n",
    "            tune=600,           # More tuning\n",
    "            chains=2,\n",
    "            cores=2,\n",
    "            random_seed=42,\n",
    "            target_accept=0.85,  # Higher for complex models\n",
    "            max_treedepth=12,    # Allow deeper trees\n",
    "            return_inferencedata=True,\n",
    "            progressbar=False,\n",
    "            nuts_sampler_kwargs={'store_unconstrained_samples': False},\n",
    "            initvals=map_est if map_est else None,  # Use MAP if available\n",
    "            discard_tuned_samples=True\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"done!\", flush=True)\n",
    "    \n",
    "    return model, idata\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN FITTING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "sv_results = {}\n",
    "sv_summary_table = {}\n",
    "sv_diagnostics = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FAST & ACCURATE STOCHASTIC VOLATILITY MODEL\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "# Track timing\n",
    "import time\n",
    "start_total = time.time()\n",
    "\n",
    "for idx, stock in enumerate(stock_list, 1):\n",
    "    print(f\"[{idx}/{len(stock_list)}] {stock}...\", end=\" \", flush=True)\n",
    "    \n",
    "    returns_train = train_df[stock].dropna().values\n",
    "    n_obs = len(returns_train)\n",
    "    \n",
    "    start_stock = time.time()\n",
    "    \n",
    "    try:\n",
    "        # ✅ FIT MODEL\n",
    "        model, idata = fit_sv_with_map_init(returns_train, stock, verbose=False)\n",
    "        \n",
    "        # ✅ EXTRACT SUMMARY STATISTICS\n",
    "        summary = az.summary(idata, var_names=['mu', 'alpha', 'phi', 'sigma_eta'])\n",
    "        \n",
    "        mu_mean = summary.loc['mu', 'mean']\n",
    "        alpha_mean = summary.loc['alpha', 'mean']\n",
    "        phi_mean = summary.loc['phi', 'mean']\n",
    "        sigma_eta_mean = summary.loc['sigma_eta', 'mean']\n",
    "        \n",
    "        # ✅ COMPUTE DERIVED QUANTITIES\n",
    "        vol_level = np.exp(alpha_mean / 2) * 100  # Long-run vol in %\n",
    "        \n",
    "        # ✅ DIAGNOSTICS\n",
    "        rhat_max = summary['r_hat'].max()\n",
    "        ess_bulk_min = summary['ess_bulk'].min()\n",
    "        ess_tail_min = summary['ess_tail'].min()\n",
    "        div_count = idata.sample_stats.divergent.sum().item()\n",
    "        \n",
    "        # ✅ MCMC DIAGNOSTICS\n",
    "        n_eff_ratio = ess_bulk_min / (2 * 800)  # 2 chains, 800 draws\n",
    "        \n",
    "        # ✅ CONVERGENCE CHECK\n",
    "        converged = (rhat_max < 1.01) and (ess_bulk_min > 400) and (div_count == 0)\n",
    "        status = \"✓\" if converged else \"⚠\"\n",
    "        \n",
    "        # ✅ STORE RESULTS\n",
    "        sv_results[stock] = {\n",
    "            'model': model,\n",
    "            'idata': idata,\n",
    "            'n_obs': n_obs\n",
    "        }\n",
    "        \n",
    "        sv_summary_table[stock] = {\n",
    "            'mu': mu_mean,\n",
    "            'alpha': alpha_mean,\n",
    "            'phi': phi_mean,\n",
    "            'sigma_eta': sigma_eta_mean,\n",
    "            'vol_pct': vol_level,\n",
    "            'rhat_max': rhat_max,\n",
    "            'ess_bulk_min': ess_bulk_min,\n",
    "            'ess_tail_min': ess_tail_min,\n",
    "            'divergences': int(div_count),\n",
    "            'eff_sample_ratio': n_eff_ratio\n",
    "        }\n",
    "        \n",
    "        sv_diagnostics[stock] = {\n",
    "            'converged': converged,\n",
    "            'rhat_ok': rhat_max < 1.01,\n",
    "            'ess_ok': ess_bulk_min > 400,\n",
    "            'div_ok': div_count == 0,\n",
    "            'eff_ok': n_eff_ratio > 0.5\n",
    "        }\n",
    "        \n",
    "        elapsed = time.time() - start_stock\n",
    "        \n",
    "        # ✅ PRINT RESULTS\n",
    "        print(f\"{status} φ={phi_mean:.4f}, σ_η={sigma_eta_mean:.4f}, Vol={vol_level:.2f}%\", end=\"\")\n",
    "        print(f\" | R̂={rhat_max:.4f}, ESS={ess_bulk_min:.0f}, Div={int(div_count)} | {elapsed:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ ERROR: {str(e)[:80]}\")\n",
    "        sv_summary_table[stock] = {'error': str(e)[:100]}\n",
    "        sv_diagnostics[stock] = {'converged': False, 'error': True}\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "elapsed_total = time.time() - start_total\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "sv_df = pd.DataFrame(sv_summary_table).T\n",
    "\n",
    "# Display key columns\n",
    "display_cols = ['phi', 'sigma_eta', 'vol_pct', 'rhat_max', 'ess_bulk_min', 'divergences', 'eff_sample_ratio']\n",
    "valid_cols = [c for c in display_cols if c in sv_df.columns]\n",
    "\n",
    "print(sv_df[valid_cols].round(4).to_string())\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGNOSTICS SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "diag_df = pd.DataFrame(sv_diagnostics).T\n",
    "print(diag_df.to_string())\n",
    "\n",
    "n_converged = diag_df['converged'].sum()\n",
    "print(f\"\\n✓ {n_converged}/{len(stock_list)} models converged successfully\")\n",
    "print(f\"✓ Total time: {elapsed_total/60:.1f} minutes ({elapsed_total/len(stock_list):.1f} min/stock)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "sv_df.to_csv(f\"{results_dirs['sv']}/sv_fast_results.csv\")\n",
    "diag_df.to_csv(f\"{results_dirs['sv']}/sv_diagnostics.csv\")\n",
    "\n",
    "print(f\"\\n✓ Results saved:\")\n",
    "print(f\"  - {results_dirs['sv']}/sv_fast_results.csv\")\n",
    "print(f\"  - {results_dirs['sv']}/sv_diagnostics.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT & STORE LATENT VOLATILITIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EXTRACTING LATENT VOLATILITIES\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "sv_latent_vols = {}\n",
    "\n",
    "for stock in sv_results.keys():\n",
    "    try:\n",
    "        idata = sv_results[stock]['idata']\n",
    "        h_seq_posterior = idata.posterior['h_seq'].values  # Shape: (chains, draws, time)\n",
    "        h_mean = h_seq_posterior.mean(axis=(0, 1))  # Average over chains and draws\n",
    "        h_std = h_seq_posterior.std(axis=(0, 1))\n",
    "        \n",
    "        vol_mean = np.exp(h_mean / 2) * 100  # Convert to %\n",
    "        vol_lower = np.exp((h_mean - 1.96*h_std) / 2) * 100\n",
    "        vol_upper = np.exp((h_mean + 1.96*h_std) / 2) * 100\n",
    "        \n",
    "        sv_latent_vols[stock] = {\n",
    "            'vol_mean': vol_mean,\n",
    "            'vol_lower': vol_lower,\n",
    "            'vol_upper': vol_upper,\n",
    "            'h_seq': h_mean\n",
    "        }\n",
    "        \n",
    "        print(f\"{stock}: mean vol={vol_mean.mean():.2f}%, range=[{vol_lower.mean():.2f}%, {vol_upper.mean():.2f}%]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{stock}: ✗ Error extracting volatility ({str(e)[:50]})\")\n",
    "\n",
    "# ============================================================================\n",
    "# QUALITY CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"QUALITY ASSURANCE\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "quality_checks = {\n",
    "    'R̂ < 1.01': (sv_df['rhat_max'] < 1.01).sum(),\n",
    "    'ESS_bulk > 400': (sv_df['ess_bulk_min'] > 400).sum(),\n",
    "    'No divergences': (sv_df['divergences'] == 0).sum(),\n",
    "    'Effective sample ratio > 0.5': (sv_df['eff_sample_ratio'] > 0.5).sum(),\n",
    "}\n",
    "\n",
    "for check, count in quality_checks.items():\n",
    "    pct = 100 * count / len(sv_df)\n",
    "    status = \"✓\" if pct >= 80 else \"⚠\"\n",
    "    print(f\"{status} {check}: {count}/{len(sv_df)} ({pct:.0f}%)\")\n",
    "\n",
    "print(\"\\n✓ SV model fitting complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
