{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997e3662-123f-4cf4-9f6e-609d6c8c8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from src.ar_models import (\n",
    "    fit_ar_frequentist, fit_ar_bayesian, fit_ar_hierarchical,\n",
    "    forecast_ar_posterior_predictive, get_ar_prior\n",
    ")\n",
    "from src.results_manager import ResultsManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60ffad8-31d5-4d3b-912e-d453a7ffe93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks: ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA']\n",
      "Prior sets: ['weak', 'medium', 'informative']\n",
      "Horizons: [1, 5, 22]\n",
      "{'phi_mean': 0.0, 'phi_std': 1.0, 'sigma2_alpha': 1.0, 'sigma2_beta': 1.0, 'description': 'Weak (diffuse)'}\n",
      "{'phi_mean': 0.0, 'phi_std': 0.2, 'sigma2_alpha': 10.0, 'sigma2_beta': 10.0, 'description': 'Medium'}\n",
      "{'phi_mean': 0.0, 'phi_std': 0.1, 'sigma2_alpha': 100.0, 'sigma2_beta': 100.0, 'description': 'Informative '}\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path('../data/processed')\n",
    "FIG_DIR = Path('../figures')\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA']  # 7 stocks\n",
    "PRIOR_SETS = ['weak', 'medium', 'informative']\n",
    "HORIZONS = [1, 5, 22]\n",
    "N_SAMPLES = 5000\n",
    "N_BURNIN = 1000\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Load data\n",
    "with open(DATA_DIR / 'train_test_split.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "results_mgr = ResultsManager()\n",
    "\n",
    "print(f\"Stocks: {TICKERS}\")\n",
    "print(f\"Prior sets: {PRIOR_SETS}\")\n",
    "print(f\"Horizons: {HORIZONS}\")\n",
    "print(get_ar_prior('weak'))\n",
    "print(get_ar_prior('medium'))\n",
    "print(get_ar_prior('informative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6720eaa2-69f8-4f13-b902-7b7825b26869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 1: AR(1) FREQUENTIST - WITH PROPER PREDICTION INTERVALS (t-distribution)\n",
      "AAPL... Saved AR: AAPL_ar_freq.pkl\n",
      "‚úì\n",
      "MSFT... Saved AR: MSFT_ar_freq.pkl\n",
      "‚úì\n",
      "GOOGL... Saved AR: GOOGL_ar_freq.pkl\n",
      "‚úì\n",
      "AMZN... Saved AR: AMZN_ar_freq.pkl\n",
      "‚úì\n",
      "TSLA... Saved AR: TSLA_ar_freq.pkl\n",
      "‚úì\n",
      "META... Saved AR: META_ar_freq.pkl\n",
      "‚úì\n",
      "NVDA... Saved AR: NVDA_ar_freq.pkl\n",
      "‚úì\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def forecast_ar_frequentist_prediction_intervals(last_train, test_data, phi, sigma2, \n",
    "                                                  n_obs, horizons=[1, 5, 22]):\n",
    "    \"\"\"\n",
    "    PROPER Frequentist Prediction Interval using Student-t distribution\n",
    "    \n",
    "    For AR(1): y_{t+h} = œÜ * y_{t+h-1} + Œµ_{t+h}\n",
    "    \n",
    "    Prediction interval accounts for:\n",
    "    1. Uncertainty in œÜ and œÉ¬≤ estimates\n",
    "    2. Future shocks\n",
    "    3. Degrees of freedom (t-distribution, not z)\n",
    "    \n",
    "    Formula for h-step ahead:\n",
    "    PI = y_hat ¬± t_{Œ±/2, n-2} * œÉ_h-step\n",
    "    \n",
    "    where œÉ_h-step = œÉ * sqrt(1 + œÜ¬≤ + œÜ‚Å¥ + ... + œÜ^(2(h-1)))\n",
    "    \"\"\"\n",
    "    forecasts = {}\n",
    "    \n",
    "    # Degrees of freedom = n - 2 (for AR(1) with intercept)\n",
    "    df = max(n_obs - 2, 1)\n",
    "    t_critical = stats.t.ppf(0.975, df)  # 95% two-tailed\n",
    "    \n",
    "    for h in horizons:\n",
    "        # Point forecast (deterministic)\n",
    "        fc_point = np.zeros(h)\n",
    "        phi_clip = np.clip(phi, -0.999, 0.999)\n",
    "        \n",
    "        if h == 1:\n",
    "            fc_point[0] = phi_clip * last_train\n",
    "        else:\n",
    "            fc_point[0] = phi_clip * last_train\n",
    "            for t in range(1, h):\n",
    "                fc_point[t] = phi_clip * fc_point[t-1]\n",
    "        \n",
    "        # Variance grows with horizon\n",
    "        sigma_h_step = np.zeros(h)\n",
    "        sigma_h_step[0] = np.sqrt(sigma2)  # 1-step: just shock variance\n",
    "        \n",
    "        for t in range(1, h):\n",
    "            # Sum of powers of œÜ¬≤\n",
    "            phi_sq = phi_clip ** 2\n",
    "            power_sum = sum([phi_sq ** k for k in range(t)])\n",
    "            sigma_h_step[t] = np.sqrt(sigma2 * (1 + power_sum))\n",
    "            \n",
    "            # Cap at reasonable value (prevent blowup)\n",
    "            sigma_h_step[t] = np.clip(sigma_h_step[t], 0, 100)\n",
    "        \n",
    "        # Prediction interval (t-critical, not z-critical!)\n",
    "        max_test_len = min(h, len(test_data))\n",
    "        actual = test_data[:max_test_len]\n",
    "        \n",
    "        fc_trunc = fc_point[:max_test_len]\n",
    "        sigma_trunc = sigma_h_step[:max_test_len]\n",
    "        \n",
    "        ci_lower = fc_trunc - t_critical * sigma_trunc\n",
    "        ci_upper = fc_trunc + t_critical * sigma_trunc\n",
    "        \n",
    "        # PIS\n",
    "        pis_scores = []\n",
    "        for t in range(len(actual)):\n",
    "            lower, upper, y = ci_lower[t], ci_upper[t], actual[t]\n",
    "            if y < lower:\n",
    "                pis = (upper - lower) + (2/0.95) * (lower - y)\n",
    "            elif y > upper:\n",
    "                pis = (upper - lower) + (2/0.95) * (y - upper)\n",
    "            else:\n",
    "                pis = (upper - lower)\n",
    "            pis_scores.append(pis)\n",
    "        \n",
    "        coverage = float(np.mean((actual >= ci_lower) & (actual <= ci_upper)))\n",
    "        mse = float(np.mean((actual - fc_trunc) ** 2))\n",
    "        \n",
    "        forecasts[f'h_{h}'] = {\n",
    "            'forecast_mean': fc_trunc.tolist(),\n",
    "            'ci_lower': ci_lower.tolist(),\n",
    "            'ci_upper': ci_upper.tolist(),\n",
    "            'actual': actual.tolist(),\n",
    "            'coverage': coverage,\n",
    "            'mse': mse,\n",
    "            'rmse': float(np.sqrt(mse)),\n",
    "            'interval_width': float(np.mean(ci_upper - ci_lower)),\n",
    "            'pis': float(np.mean(pis_scores)),\n",
    "            'df': int(df),\n",
    "            't_critical': float(t_critical)\n",
    "        }\n",
    "    \n",
    "    return forecasts\n",
    "\n",
    "print(\"PART 1: AR(1) FREQUENTIST - WITH PROPER PREDICTION INTERVALS (t-distribution)\")\n",
    "\n",
    "freq_results = {}\n",
    "for ticker in TICKERS:\n",
    "    train = data['train'][ticker].values\n",
    "    test = data['test'][ticker].values\n",
    "    \n",
    "    print(f\"{ticker}...\", end=\" \")\n",
    "    \n",
    "    # Fit\n",
    "    result = fit_ar_frequentist(train)\n",
    "    freq_results[ticker] = result\n",
    "    \n",
    "    # Use PROPER prediction intervals (not asymptotic)\n",
    "    forecasts = forecast_ar_frequentist_prediction_intervals(\n",
    "        last_train=train[-1],\n",
    "        test_data=test,\n",
    "        phi=result['phi'],\n",
    "        sigma2=result['sigma2'],\n",
    "        n_obs=len(train),\n",
    "        horizons=HORIZONS\n",
    "    )\n",
    "    \n",
    "    freq_results[ticker]['forecasts'] = forecasts\n",
    "    \n",
    "    # Save\n",
    "    results_mgr.save_ar(ticker, 'freq', result)\n",
    "    print(f\"‚úì\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051cf25f-cb35-4c62-8471-65a5bb7e3895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 2: AR(1) BAYESIAN (Gibbs) - WITH PREDICTION INTERVALS\n",
      "AAPL + weak... Saved AR: AAPL_ar_bayes_weak.pkl\n",
      "AAPL + medium... Saved AR: AAPL_ar_bayes_medium.pkl\n",
      "AAPL + informative... Saved AR: AAPL_ar_bayes_informative.pkl\n",
      "MSFT + weak... Saved AR: MSFT_ar_bayes_weak.pkl\n",
      "MSFT + medium... Saved AR: MSFT_ar_bayes_medium.pkl\n",
      "MSFT + informative... Saved AR: MSFT_ar_bayes_informative.pkl\n",
      "GOOGL + weak... Saved AR: GOOGL_ar_bayes_weak.pkl\n",
      "GOOGL + medium... Saved AR: GOOGL_ar_bayes_medium.pkl\n",
      "GOOGL + informative... Saved AR: GOOGL_ar_bayes_informative.pkl\n",
      "AMZN + weak... Saved AR: AMZN_ar_bayes_weak.pkl\n",
      "AMZN + medium... Saved AR: AMZN_ar_bayes_medium.pkl\n",
      "AMZN + informative... Saved AR: AMZN_ar_bayes_informative.pkl\n",
      "TSLA + weak... Saved AR: TSLA_ar_bayes_weak.pkl\n",
      "TSLA + medium... Saved AR: TSLA_ar_bayes_medium.pkl\n",
      "TSLA + informative... Saved AR: TSLA_ar_bayes_informative.pkl\n",
      "META + weak... Saved AR: META_ar_bayes_weak.pkl\n",
      "META + medium... Saved AR: META_ar_bayes_medium.pkl\n",
      "META + informative... Saved AR: META_ar_bayes_informative.pkl\n",
      "NVDA + weak... Saved AR: NVDA_ar_bayes_weak.pkl\n",
      "NVDA + medium... Saved AR: NVDA_ar_bayes_medium.pkl\n",
      "NVDA + informative... Saved AR: NVDA_ar_bayes_informative.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"PART 2: AR(1) BAYESIAN (Gibbs) - WITH PREDICTION INTERVALS\")\n",
    "\n",
    "bayes_results = {}\n",
    "for ticker in TICKERS:\n",
    "    bayes_results[ticker] = {}\n",
    "    train = data['train'][ticker].values\n",
    "    test = data['test'][ticker].values\n",
    "    \n",
    "    for prior_set in PRIOR_SETS:\n",
    "        print(f\"{ticker} + {prior_set}...\", end=\" \")\n",
    "        \n",
    "        # Fit\n",
    "        result = fit_ar_bayesian(train, prior_set=prior_set, \n",
    "                                n_samples=N_SAMPLES, n_burnin=N_BURNIN)\n",
    "        bayes_results[ticker][prior_set] = result\n",
    "        \n",
    "        # Forecast using POSTERIOR SAMPLES (generates CI)\n",
    "        forecasts = forecast_ar_posterior_predictive(\n",
    "            last_train=train[-1],\n",
    "            test_data=test,\n",
    "            phi_samples=result['phi_samples'],\n",
    "            sigma2_samples=result['sigma2_samples'],\n",
    "            horizons=HORIZONS\n",
    "        )\n",
    "        bayes_results[ticker][prior_set]['forecasts'] = forecasts\n",
    "        \n",
    "        # Save\n",
    "        results_mgr.save_ar(ticker, 'bayes', result, prior_set=prior_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8c1539-a425-44d5-bec7-bd7fc205b2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 3: AR(1) HIERARCHICAL (All 7 stocks together)\n",
      "Fitting hierarchical model... Informative Hierarchical AR(1) Priors:\n",
      "  œÜ group mean Œº_œÜ: N(0, 0.01)\n",
      "  œÜ group SD œÑ_œÜ¬≤: Gamma(100.0, 100.0)\n",
      "  œÉ¬≤_k: Gamma(100.0, 100.0)\n",
      "  Initial œÑ_œÜ: 0.05\n",
      "\n",
      "Done! Time=3.9s\n",
      "AAPL... MSFT... GOOGL... AMZN... TSLA... META... NVDA... Saved Hierarchical AR: hierarchical_ar.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('../results/hierarchical/hierarchical_ar.pkl')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"PART 3: AR(1) HIERARCHICAL (All 7 stocks together)\")\n",
    "\n",
    "# Prepare data dict\n",
    "data_dict = {ticker: data['train'][ticker].values for ticker in TICKERS}\n",
    "\n",
    "print(\"Fitting hierarchical model...\", end=\" \")\n",
    "hier_result = fit_ar_hierarchical(data_dict, n_samples=N_SAMPLES, n_burnin=N_BURNIN)\n",
    "print(f\"Done! Time={hier_result['meta']['runtime']:.1f}s\")\n",
    "\n",
    "# Forecast each ticker using POSTERIOR SAMPLES\n",
    "hier_forecasts = {}\n",
    "for ticker in TICKERS:\n",
    "    test = data['test'][ticker].values\n",
    "    train = data['train'][ticker].values\n",
    "    \n",
    "    # Use hierarchical posterior samples\n",
    "    phi_samples = hier_result[ticker]['phi_samples']\n",
    "    sigma2_samples = hier_result[ticker]['sigma2_samples']\n",
    "    \n",
    "    forecasts = forecast_ar_posterior_predictive(\n",
    "        last_train=train[-1],\n",
    "        test_data=test,\n",
    "        phi_samples=phi_samples,\n",
    "        sigma2_samples=sigma2_samples,\n",
    "        horizons=HORIZONS\n",
    "    )\n",
    "    hier_forecasts[ticker] = forecasts\n",
    "    print(f\"{ticker}...\", end=\" \")\n",
    "\n",
    "\n",
    "results_mgr.save_hierarchical(model_type='ar', results=hier_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6accb0ac-9573-412b-972d-56b4dad018b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 4: EVALUATION TABLE - PIS + COVERAGE\n",
      "================================================================================\n",
      "\n",
      "FULL EVALUATION (PIS = Primary Metric):\n",
      "\n",
      "   Model        Prior  Horizon  Coverage     PIS  Interval_Width    MSE\n",
      " AR-Freq          N/A        1    1.0000  0.0659          0.0659 0.0001\n",
      " AR-Freq          N/A        5    1.0000  0.0878          0.0878 0.0002\n",
      " AR-Freq          N/A       22    1.0000  0.0920          0.0920 0.0003\n",
      "AR-Bayes         weak        1    1.0000 69.6652         69.6652 0.0170\n",
      "AR-Bayes         weak        5    1.0000 92.8422         92.8422 0.3672\n",
      "AR-Bayes         weak       22    1.0000 97.1994         97.1994 0.5607\n",
      "AR-Bayes       medium        1    1.0000 24.1539         24.1539 0.0154\n",
      "AR-Bayes       medium        5    1.0000 25.9346         25.9346 0.0099\n",
      "AR-Bayes       medium       22    1.0000 25.8290         25.8290 0.0498\n",
      "AR-Bayes  informative        1    1.0000  9.6348          9.6348 0.0028\n",
      "AR-Bayes  informative        5    1.0000  9.7328          9.7328 0.0041\n",
      "AR-Bayes  informative       22    1.0000  9.7897          9.7897 0.0067\n",
      " AR-Hier Hierarchical        1    1.0000  9.3059          9.3059 0.0033\n",
      " AR-Hier Hierarchical        5    1.0000 13.5733         13.5733 0.0063\n",
      " AR-Hier Hierarchical       22    1.0000 15.0507         15.0507 0.0167\n",
      " AR-Freq          N/A        1    1.0000  0.0639          0.0639 0.0001\n",
      " AR-Freq          N/A        5    1.0000  0.0850          0.0850 0.0004\n",
      " AR-Freq          N/A       22    0.9545  0.0909          0.0891 0.0004\n",
      "AR-Bayes         weak        1    1.0000 72.8134         72.8134 0.0292\n",
      "AR-Bayes         weak        5    1.0000 90.0754         90.0754 0.3362\n",
      "AR-Bayes         weak       22    1.0000 93.6279         93.6279 0.3024\n",
      "AR-Bayes       medium        1    1.0000 25.0347         25.0347 0.0063\n",
      "AR-Bayes       medium        5    1.0000 26.0934         26.0934 0.0485\n",
      "AR-Bayes       medium       22    1.0000 26.0627         26.0627 0.0595\n",
      "AR-Bayes  informative        1    1.0000  9.6211          9.6211 0.0022\n",
      "AR-Bayes  informative        5    1.0000  9.7786          9.7786 0.0082\n",
      "AR-Bayes  informative       22    1.0000  9.7800          9.7800 0.0054\n",
      " AR-Hier Hierarchical        1    1.0000  9.5738          9.5738 0.0008\n",
      " AR-Hier Hierarchical        5    1.0000 13.6525         13.6525 0.0045\n",
      " AR-Hier Hierarchical       22    1.0000 14.8509         14.8509 0.0116\n",
      " AR-Freq          N/A        1    1.0000  0.0755          0.0755 0.0003\n",
      " AR-Freq          N/A        5    1.0000  0.1005          0.1005 0.0001\n",
      " AR-Freq          N/A       22    0.9545  0.1075          0.1054 0.0005\n",
      "AR-Bayes         weak        1    1.0000 64.3812         64.3812 0.0162\n",
      "AR-Bayes         weak        5    1.0000 87.1024         87.1024 0.4257\n",
      "AR-Bayes         weak       22    1.0000 91.6913         91.6913 0.2116\n",
      "AR-Bayes       medium        1    1.0000 25.1942         25.1942 0.0127\n",
      "AR-Bayes       medium        5    1.0000 25.5540         25.5540 0.0212\n",
      "AR-Bayes       medium       22    1.0000 25.7723         25.7723 0.0386\n",
      "AR-Bayes  informative        1    1.0000  9.8238          9.8238 0.0007\n",
      "AR-Bayes  informative        5    1.0000  9.8158          9.8158 0.0026\n",
      "AR-Bayes  informative       22    1.0000  9.7193          9.7193 0.0061\n",
      " AR-Hier Hierarchical        1    1.0000  9.4606          9.4606 0.0112\n",
      " AR-Hier Hierarchical        5    1.0000 13.5861         13.5861 0.0169\n",
      " AR-Hier Hierarchical       22    1.0000 14.7407         14.7407 0.0063\n",
      " AR-Freq          N/A        1    1.0000  0.0864          0.0864 0.0006\n",
      " AR-Freq          N/A        5    1.0000  0.1150          0.1150 0.0003\n",
      " AR-Freq          N/A       22    1.0000  0.1205          0.1205 0.0002\n",
      "AR-Bayes         weak        1    1.0000 68.7133         68.7133 0.0074\n",
      "AR-Bayes         weak        5    1.0000 83.1201         83.1201 0.2584\n",
      "AR-Bayes         weak       22    1.0000 88.7246         88.7246 0.4753\n",
      "AR-Bayes       medium        1    1.0000 24.7771         24.7771 0.0342\n",
      "AR-Bayes       medium        5    1.0000 25.8845         25.8845 0.0173\n",
      "AR-Bayes       medium       22    1.0000 25.6866         25.6866 0.0689\n",
      "AR-Bayes  informative        1    1.0000  9.8831          9.8831 0.0199\n",
      "AR-Bayes  informative        5    1.0000  9.6335          9.6335 0.0062\n",
      "AR-Bayes  informative       22    1.0000  9.7436          9.7436 0.0052\n",
      " AR-Hier Hierarchical        1    1.0000  9.4270          9.4270 0.0224\n",
      " AR-Hier Hierarchical        5    1.0000 12.9669         12.9669 0.0079\n",
      " AR-Hier Hierarchical       22    1.0000 15.0025         15.0025 0.0355\n",
      " AR-Freq          N/A        1    1.0000  0.1480          0.1480 0.0009\n",
      " AR-Freq          N/A        5    1.0000  0.1971          0.1971 0.0003\n",
      " AR-Freq          N/A       22    1.0000  0.2066          0.2066 0.0009\n",
      "AR-Bayes         weak        1    1.0000 57.9371         57.9371 0.5140\n",
      "AR-Bayes         weak        5    1.0000 64.8260         64.8260 0.4243\n",
      "AR-Bayes         weak       22    1.0000 68.6188         68.6188 0.1654\n",
      "AR-Bayes       medium        1    1.0000 23.3962         23.3962 0.0002\n",
      "AR-Bayes       medium        5    1.0000 24.6140         24.6140 0.0314\n",
      "AR-Bayes       medium       22    1.0000 24.9529         24.9529 0.0492\n",
      "AR-Bayes  informative        1    1.0000  9.7449          9.7449 0.0055\n",
      "AR-Bayes  informative        5    1.0000  9.9886          9.9886 0.0043\n",
      "AR-Bayes  informative       22    1.0000  9.6809          9.6809 0.0070\n",
      " AR-Hier Hierarchical        1    1.0000  9.4316          9.4316 0.0045\n",
      " AR-Hier Hierarchical        5    1.0000 13.1173         13.1173 0.0120\n",
      " AR-Hier Hierarchical       22    1.0000 14.8151         14.8151 0.0088\n",
      " AR-Freq          N/A        1    1.0000  0.1108          0.1108 0.0000\n",
      " AR-Freq          N/A        5    1.0000  0.1475          0.1475 0.0002\n",
      " AR-Freq          N/A       22    1.0000  0.1545          0.1545 0.0002\n",
      "AR-Bayes         weak        1    1.0000 60.4254         60.4254 0.1028\n",
      "AR-Bayes         weak        5    1.0000 77.1349         77.1349 0.9709\n",
      "AR-Bayes         weak       22    1.0000 78.9685         78.9685 0.4346\n",
      "AR-Bayes       medium        1    1.0000 25.0406         25.0406 0.0543\n",
      "AR-Bayes       medium        5    1.0000 25.4772         25.4772 0.0108\n",
      "AR-Bayes       medium       22    1.0000 25.6259         25.6259 0.0583\n",
      "AR-Bayes  informative        1    1.0000  9.4064          9.4064 0.0034\n",
      "AR-Bayes  informative        5    1.0000  9.7217          9.7217 0.0042\n",
      "AR-Bayes  informative       22    1.0000  9.7906          9.7906 0.0078\n",
      " AR-Hier Hierarchical        1    1.0000  9.3347          9.3347 0.0089\n",
      " AR-Hier Hierarchical        5    1.0000 13.2774         13.2774 0.0096\n",
      " AR-Hier Hierarchical       22    1.0000 15.2261         15.2261 0.0079\n",
      " AR-Freq          N/A        1    1.0000  0.1280          0.1280 0.0009\n",
      " AR-Freq          N/A        5    1.0000  0.1705          0.1705 0.0009\n",
      " AR-Freq          N/A       22    0.9545  0.1878          0.1787 0.0026\n",
      "AR-Bayes         weak        1    1.0000 55.8228         55.8228 0.1106\n",
      "AR-Bayes         weak        5    1.0000 69.9003         69.9003 0.0836\n",
      "AR-Bayes         weak       22    1.0000 73.3404         73.3404 0.3768\n",
      "AR-Bayes       medium        1    1.0000 25.0430         25.0430 0.0015\n",
      "AR-Bayes       medium        5    1.0000 24.7724         24.7724 0.0050\n",
      "AR-Bayes       medium       22    1.0000 25.3984         25.3984 0.0599\n",
      "AR-Bayes  informative        1    1.0000  9.5408          9.5408 0.0053\n",
      "AR-Bayes  informative        5    1.0000  9.8071          9.8071 0.0066\n",
      "AR-Bayes  informative       22    1.0000  9.6816          9.6816 0.0114\n",
      " AR-Hier Hierarchical        1    1.0000  9.4264          9.4264 0.0027\n",
      " AR-Hier Hierarchical        5    1.0000 12.4591         12.4591 0.0254\n",
      " AR-Hier Hierarchical       22    1.0000 14.3885         14.3885 0.0232\n",
      "\n",
      "‚úì Saved: ar_evaluation_pis.csv\n",
      "\n",
      "================================================================================\n",
      "INTERVAL WIDTH COMPARISON (Frequentist vs Bayesian)\n",
      "================================================================================\n",
      "\n",
      " Horizon  Freq_Width  Bayes_Weak_Width  Bayes_Medium_Width  Freq_vs_Weak_Ratio  Freq_vs_Medium_Ratio\n",
      "       1      0.0969           64.2512             24.6628              0.0015                0.0039\n",
      "       5      0.1291           80.7145             25.4757              0.0016                0.0051\n",
      "      22      0.1353           84.5958             25.6183              0.0016                0.0053\n",
      "\n",
      "Expected: Frequentist should be NARROWER than Bayesian (less info)\n",
      "          Ratio < 1.0 indicates Freq narrower (better)\n",
      "\n",
      "================================================================================\n",
      "PIS SUMMARY (LOWER = BETTER)\n",
      "================================================================================\n",
      "\n",
      "   Model        Prior  Horizon     PIS  Coverage    MSE  Interval_Width\n",
      "AR-Bayes  informative        1  9.6650    1.0000 0.0057          9.6650\n",
      "AR-Bayes  informative        5  9.7826    1.0000 0.0052          9.7826\n",
      "AR-Bayes  informative       22  9.7408    1.0000 0.0071          9.7408\n",
      "AR-Bayes       medium        1 24.6628    1.0000 0.0178         24.6628\n",
      "AR-Bayes       medium        5 25.4757    1.0000 0.0206         25.4757\n",
      "AR-Bayes       medium       22 25.6183    1.0000 0.0549         25.6183\n",
      "AR-Bayes         weak        1 64.2512    1.0000 0.1139         64.2512\n",
      "AR-Bayes         weak        5 80.7145    1.0000 0.4095         80.7145\n",
      "AR-Bayes         weak       22 84.5958    1.0000 0.3610         84.5958\n",
      " AR-Freq          N/A        1  0.0969    1.0000 0.0004          0.0969\n",
      " AR-Freq          N/A        5  0.1291    1.0000 0.0003          0.1291\n",
      " AR-Freq          N/A       22  0.1371    0.9805 0.0007          0.1353\n",
      " AR-Hier Hierarchical        1  9.4229    1.0000 0.0077          9.4229\n",
      " AR-Hier Hierarchical        5 13.2332    1.0000 0.0118         13.2332\n",
      " AR-Hier Hierarchical       22 14.8678    1.0000 0.0157         14.8678\n",
      "\n",
      "================================================================================\n",
      "üèÜ BEST MODEL BY PIS (LOWER BETTER)\n",
      "================================================================================\n",
      "\n",
      "h= 1: AR-Freq    + N/A          | PIS=0.0639 | Cov=1.0000\n",
      "h= 5: AR-Freq    + N/A          | PIS=0.0850 | Cov=1.0000\n",
      "h=22: AR-Freq    + N/A          | PIS=0.0909 | Cov=0.9545\n",
      "\n",
      "‚úì All saved!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 4: EVALUATION TABLE - PIS + COVERAGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "evaluation_data = []\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    # Frequentist\n",
    "    for h in HORIZONS:\n",
    "        fc = freq_results[ticker]['forecasts'][f'h_{h}']\n",
    "        evaluation_data.append({\n",
    "            'Stock': ticker,\n",
    "            'Model': 'AR-Freq',\n",
    "            'Prior': 'N/A',\n",
    "            'Horizon': h,\n",
    "            'Coverage': fc['coverage'],\n",
    "            'MSE': fc['mse'],\n",
    "            'RMSE': fc['rmse'],\n",
    "            'Interval_Width': fc['interval_width'],\n",
    "            'PIS': fc['pis'],\n",
    "            'df': fc.get('df', np.nan),\n",
    "            't_critical': fc.get('t_critical', np.nan)\n",
    "        })\n",
    "    \n",
    "    # Bayesian\n",
    "    for prior_set in PRIOR_SETS:\n",
    "        for h in HORIZONS:\n",
    "            fc = bayes_results[ticker][prior_set]['forecasts'][f'h_{h}']\n",
    "            evaluation_data.append({\n",
    "                'Stock': ticker,\n",
    "                'Model': 'AR-Bayes',\n",
    "                'Prior': prior_set,\n",
    "                'Horizon': h,\n",
    "                'Coverage': fc['coverage'],\n",
    "                'MSE': fc['mse'],\n",
    "                'RMSE': fc['rmse'],\n",
    "                'Interval_Width': fc['interval_width'],\n",
    "                'PIS': fc['pis'],\n",
    "                'df': np.nan,\n",
    "                't_critical': np.nan\n",
    "            })\n",
    "    \n",
    "    # Hierarchical\n",
    "    for h in HORIZONS:\n",
    "        fc = hier_forecasts[ticker][f'h_{h}']\n",
    "        evaluation_data.append({\n",
    "            'Stock': ticker,\n",
    "            'Model': 'AR-Hier',\n",
    "            'Prior': 'Hierarchical',\n",
    "            'Horizon': h,\n",
    "            'Coverage': fc['coverage'],\n",
    "            'MSE': fc['mse'],\n",
    "            'RMSE': fc['rmse'],\n",
    "            'Interval_Width': fc['interval_width'],\n",
    "            'PIS': fc['pis'],\n",
    "            'df': np.nan,\n",
    "            't_critical': np.nan\n",
    "        })\n",
    "\n",
    "# ‚úÖ CONVERT TO DATAFRAME FIRST (was missing!)\n",
    "eval_df = pd.DataFrame(evaluation_data)\n",
    "\n",
    "# Display\n",
    "display_cols = ['Model', 'Prior', 'Horizon', 'Coverage', 'PIS', 'Interval_Width', 'MSE']\n",
    "print(\"\\nFULL EVALUATION (PIS = Primary Metric):\\n\")\n",
    "print(eval_df[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "# Save\n",
    "eval_df.to_csv(FIG_DIR / 'ar_evaluation_pis.csv', index=False)\n",
    "print(f\"\\n‚úì Saved: ar_evaluation_pis.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERVAL WIDTH COMPARISON (Frequentist vs Bayesian)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "comparison = []\n",
    "for h in HORIZONS:\n",
    "    # ‚úÖ NOW use eval_df (which is a DataFrame)\n",
    "    freq_width = eval_df[(eval_df['Model'] == 'AR-Freq') & (eval_df['Horizon'] == h)]['Interval_Width'].mean()\n",
    "    bayes_weak_width = eval_df[(eval_df['Model'] == 'AR-Bayes') & (eval_df['Prior'] == 'weak') & (eval_df['Horizon'] == h)]['Interval_Width'].mean()\n",
    "    bayes_med_width = eval_df[(eval_df['Model'] == 'AR-Bayes') & (eval_df['Prior'] == 'medium') & (eval_df['Horizon'] == h)]['Interval_Width'].mean()\n",
    "    \n",
    "    comparison.append({\n",
    "        'Horizon': h,\n",
    "        'Freq_Width': float(freq_width),\n",
    "        'Bayes_Weak_Width': float(bayes_weak_width),\n",
    "        'Bayes_Medium_Width': float(bayes_med_width),\n",
    "        'Freq_vs_Weak_Ratio': float(freq_width / bayes_weak_width) if bayes_weak_width > 0 else np.nan,\n",
    "        'Freq_vs_Medium_Ratio': float(freq_width / bayes_med_width) if bayes_med_width > 0 else np.nan\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison)\n",
    "print(comp_df.round(4).to_string(index=False))\n",
    "print(\"\\nExpected: Frequentist should be NARROWER than Bayesian (less info)\")\n",
    "print(\"          Ratio < 1.0 indicates Freq narrower (better)\")\n",
    "\n",
    "comp_df.to_csv(FIG_DIR / 'ar_interval_width_comparison.csv', index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY: PIS by Model & Horizon (LOWER IS BETTER)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIS SUMMARY (LOWER = BETTER)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "pis_summary = eval_df.groupby(['Model', 'Prior', 'Horizon']).agg({\n",
    "    'PIS': 'mean',\n",
    "    'Coverage': 'mean',\n",
    "    'MSE': 'mean',\n",
    "    'Interval_Width': 'mean'\n",
    "}).round(4).reset_index()\n",
    "\n",
    "print(pis_summary.to_string(index=False))\n",
    "pis_summary.to_csv(FIG_DIR / 'ar_pis_summary.csv', index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# BEST BY PIS (not coverage)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ BEST MODEL BY PIS (LOWER BETTER)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "best_by_pis = []\n",
    "for h in HORIZONS:\n",
    "    h_data = eval_df[eval_df['Horizon'] == h].copy()\n",
    "    best = h_data.loc[h_data['PIS'].idxmin()]\n",
    "    best_by_pis.append({\n",
    "        'Horizon': h,\n",
    "        'Best_Model': best['Model'],\n",
    "        'Best_Prior': best['Prior'],\n",
    "        'PIS': best['PIS'],\n",
    "        'Coverage': best['Coverage']\n",
    "    })\n",
    "    \n",
    "    print(f\"h={h:2d}: {best['Model']:10s} + {best['Prior']:12s} | PIS={best['PIS']:.4f} | Cov={best['Coverage']:.4f}\")\n",
    "\n",
    "best_pis_df = pd.DataFrame(best_by_pis)\n",
    "best_pis_df.to_csv(FIG_DIR / 'ar_best_by_pis.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úì All saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e2b640-302b-4cfd-835f-98a5436fc075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 5: PARAMETER ESTIMATES (œÜ) COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Stock    Model        Prior  Phi_Mean  Phi_StdErr\n",
      " AAPL  AR-Freq          N/A  0.001724    0.031145\n",
      " AAPL AR-Bayes         weak -0.007311    0.997131\n",
      " AAPL AR-Bayes       medium -0.001557    0.200460\n",
      " AAPL AR-Bayes  informative  0.000663    0.099597\n",
      " AAPL  AR-Hier Hierarchical  0.012470    0.970932\n",
      " MSFT  AR-Freq          N/A -0.017063    0.031050\n",
      " MSFT AR-Bayes         weak  0.001021    0.991460\n",
      " MSFT AR-Bayes       medium -0.004803    0.198568\n",
      " MSFT AR-Bayes  informative -0.002132    0.100684\n",
      " MSFT  AR-Hier Hierarchical  0.018440    0.982728\n",
      "GOOGL  AR-Freq          N/A  0.002749    0.031062\n",
      "GOOGL AR-Bayes         weak  0.017637    1.000179\n",
      "GOOGL AR-Bayes       medium  0.002749    0.199056\n",
      "GOOGL AR-Bayes  informative  0.002761    0.100666\n",
      "GOOGL  AR-Hier Hierarchical  0.025263    0.980573\n",
      " AMZN  AR-Freq          N/A  0.001654    0.031045\n",
      " AMZN AR-Bayes         weak -0.009839    0.995485\n",
      " AMZN AR-Bayes       medium  0.004150    0.202644\n",
      " AMZN AR-Bayes  informative  0.001596    0.099449\n",
      " AMZN  AR-Hier Hierarchical  0.011027    0.971805\n",
      " TSLA  AR-Freq          N/A -0.028783    0.031043\n",
      " TSLA AR-Bayes         weak  0.018219    0.998882\n",
      " TSLA AR-Bayes       medium  0.001124    0.199976\n",
      " TSLA AR-Bayes  informative  0.000357    0.100789\n",
      " TSLA  AR-Hier Hierarchical -0.032858    0.888894\n",
      " META  AR-Freq          N/A -0.008494    0.031054\n",
      " META AR-Bayes         weak  0.031192    0.993936\n",
      " META AR-Bayes       medium -0.002209    0.202119\n",
      " META AR-Bayes  informative -0.001926    0.099251\n",
      " META  AR-Hier Hierarchical -0.013554    0.929829\n",
      " NVDA  AR-Freq          N/A -0.028062    0.031043\n",
      " NVDA AR-Bayes         weak -0.002127    0.993834\n",
      " NVDA AR-Bayes       medium -0.004792    0.199130\n",
      " NVDA AR-Bayes  informative  0.001048    0.100627\n",
      " NVDA  AR-Hier Hierarchical  0.017163    0.905089\n",
      "\n",
      "‚úì Saved: ar_phi_estimates.csv\n"
     ]
    }
   ],
   "source": [
    "# Parameter estimates comparison (not forecasts)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 5: PARAMETER ESTIMATES (œÜ) COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "phi_data = []\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    # Frequentist\n",
    "    phi_data.append({\n",
    "        'Stock': ticker,\n",
    "        'Model': 'AR-Freq',\n",
    "        'Prior': 'N/A',\n",
    "        'Phi_Mean': freq_results[ticker]['phi'],\n",
    "        'Phi_StdErr': freq_results[ticker]['se_phi']\n",
    "    })\n",
    "    \n",
    "    # Bayesian (all priors)\n",
    "    for prior_set in PRIOR_SETS:\n",
    "        phi_data.append({\n",
    "            'Stock': ticker,\n",
    "            'Model': 'AR-Bayes',\n",
    "            'Prior': prior_set,\n",
    "            'Phi_Mean': bayes_results[ticker][prior_set]['phi_mean'],\n",
    "            'Phi_StdErr': bayes_results[ticker][prior_set]['phi_std']\n",
    "        })\n",
    "    \n",
    "    # Hierarchical\n",
    "    phi_data.append({\n",
    "        'Stock': ticker,\n",
    "        'Model': 'AR-Hier',\n",
    "        'Prior': 'Hierarchical',\n",
    "        'Phi_Mean': hier_result[ticker]['phi_mean'],\n",
    "        'Phi_StdErr': hier_result[ticker]['phi_std']\n",
    "    })\n",
    "\n",
    "phi_df = pd.DataFrame(phi_data)\n",
    "print(phi_df.to_string(index=False))\n",
    "phi_df.to_csv(FIG_DIR / 'ar_phi_estimates.csv', index=False)\n",
    "print(f\"\\n‚úì Saved: ar_phi_estimates.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcb14ee6-7e6c-49a2-9aee-59f9fadfd7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLOTTING\n",
      "\n",
      "================================================================================\n",
      "PLOTTING EVALUATION METRICS\n",
      "================================================================================\n",
      "‚úì Saved: ar_coverage_by_horizon.png\n",
      "‚úì Saved: ar_mse_by_horizon.png\n",
      "‚úì Saved: ar_pis_by_horizon.png\n"
     ]
    }
   ],
   "source": [
    "print(\"PLOTTING\")\n",
    "\n",
    "# PLOT 1: MSE by Horizon (across all models)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PLOTTING EVALUATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# PLOT 1: COVERAGE BY MODEL & HORIZON\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, h in enumerate(HORIZONS):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get coverage for this horizon\n",
    "    h_data = eval_df[eval_df['Horizon'] == h]\n",
    "    \n",
    "    # Pivot: rows=Stock, cols=Model\n",
    "    pivot_cov = h_data.pivot_table(\n",
    "        values='Coverage', \n",
    "        index='Stock', \n",
    "        columns='Model',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    pivot_cov.plot(kind='bar', ax=ax, width=0.8, alpha=0.8)\n",
    "    ax.axhline(y=0.90, color='red', linestyle='--', linewidth=2, label='Target (90%)')\n",
    "    ax.set_title(f'Coverage by Model (h={h} days)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Stock')\n",
    "    ax.set_ylabel('Coverage Probability')\n",
    "    ax.set_ylim([0.70, 1.0])\n",
    "    ax.legend(title='Model', fontsize=8, loc='lower left')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'ar_coverage_by_horizon.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: ar_coverage_by_horizon.png\")\n",
    "plt.close()\n",
    "\n",
    "# PLOT 2: MSE BY MODEL & HORIZON\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, h in enumerate(HORIZONS):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    h_data = eval_df[eval_df['Horizon'] == h]\n",
    "    pivot_mse = h_data.pivot_table(\n",
    "        values='MSE', \n",
    "        index='Stock', \n",
    "        columns='Model',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    pivot_mse.plot(kind='bar', ax=ax, width=0.8, alpha=0.8)\n",
    "    ax.set_title(f'MSE by Model (h={h} days)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Stock')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.legend(title='Model', fontsize=8)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'ar_mse_by_horizon.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: ar_mse_by_horizon.png\")\n",
    "plt.close()\n",
    "\n",
    "# PLOT 3: PIS BY MODEL & HORIZON (LOWER IS BETTER)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, h in enumerate(HORIZONS):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    h_data = eval_df[eval_df['Horizon'] == h]\n",
    "    pivot_pis = h_data.pivot_table(\n",
    "        values='PIS', \n",
    "        index='Stock', \n",
    "        columns='Model',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Invert colors: lower PIS = better (darker green)\n",
    "    colors = plt.cm.RdYlGn_r(np.linspace(0.3, 1, len(pivot_pis.columns)))\n",
    "    pivot_pis.plot(kind='bar', ax=ax, width=0.8, alpha=0.8, color=colors)\n",
    "    \n",
    "    ax.set_title(f'PIS by Model (h={h} days)\\nLower=BETTER', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Stock')\n",
    "    ax.set_ylabel('Prediction Interval Score')\n",
    "    ax.legend(title='Model', fontsize=8)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'ar_pis_by_horizon.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: ar_pis_by_horizon.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a42f3-152d-4b74-934c-cf1658aff4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbad7025-7a91-4176-9096-0ee4a201c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC: BAYESIAN POSTERIOR SAMPLES\n",
      "================================================================================\n",
      "\n",
      "weak prior:\n",
      "  œÜ posterior: mean=-0.0073, std=0.9971\n",
      "             min=-3.7973, max=3.8783\n",
      "             median=-0.0166\n",
      "             pct_negative = 50.62%\n",
      "  œÉ¬≤ posterior: mean=411.594241, std=52.188044\n",
      "              min=155.552636, max=521.479795\n",
      "  œÜ * last_train: mean=0.0011, std=0.0365\n",
      "                 min=-0.0728, max=0.1094\n",
      "\n",
      "medium prior:\n",
      "  œÜ posterior: mean=-0.0016, std=0.2005\n",
      "             min=-0.8222, max=0.7729\n",
      "             median=0.0009\n",
      "             pct_negative = 49.88%\n",
      "  œÉ¬≤ posterior: mean=52.171012, std=2.325450\n",
      "              min=42.800923, max=60.286188\n",
      "  œÜ * last_train: mean=0.0003, std=0.0084\n",
      "                 min=-0.0195, max=0.0171\n",
      "\n",
      "informative prior:\n",
      "  œÜ posterior: mean=0.0007, std=0.0996\n",
      "             min=-0.3612, max=0.3924\n",
      "             median=0.0013\n",
      "             pct_negative = 49.60%\n",
      "  œÉ¬≤ posterior: mean=6.190925, std=0.249069\n",
      "              min=5.398698, max=7.192868\n",
      "  œÜ * last_train: mean=-0.0003, std=0.0040\n",
      "                 min=-0.0094, max=0.0098\n",
      "\n",
      "Frequentist œÜ = 0.0017\n",
      "Frequentist œÉ¬≤ = 0.000282\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC: BAYESIAN POSTERIOR SAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ticker = 'AAPL'\n",
    "train = data['train'][ticker].values\n",
    "test = data['test'][ticker].values\n",
    "\n",
    "# Check what posterior samples look like\n",
    "for prior_set in ['weak', 'medium', 'informative']:\n",
    "    phi_samples = bayes_results[ticker][prior_set]['phi_samples']\n",
    "    sigma2_samples = bayes_results[ticker][prior_set]['sigma2_samples']\n",
    "    \n",
    "    print(f\"\\n{prior_set} prior:\")\n",
    "    print(f\"  œÜ posterior: mean={np.mean(phi_samples):.4f}, std={np.std(phi_samples):.4f}\")\n",
    "    print(f\"             min={np.min(phi_samples):.4f}, max={np.max(phi_samples):.4f}\")\n",
    "    print(f\"             median={np.median(phi_samples):.4f}\")\n",
    "    print(f\"             pct_negative = {np.mean(phi_samples < 0):.2%}\")\n",
    "    \n",
    "    print(f\"  œÉ¬≤ posterior: mean={np.mean(sigma2_samples):.6f}, std={np.std(sigma2_samples):.6f}\")\n",
    "    print(f\"              min={np.min(sigma2_samples):.6f}, max={np.max(sigma2_samples):.6f}\")\n",
    "    \n",
    "    # What does posterior predict for first step?\n",
    "    fc_first = phi_samples[:100] * train[-1]  # First 100 samples\n",
    "    print(f\"  œÜ * last_train: mean={np.mean(fc_first):.4f}, std={np.std(fc_first):.4f}\")\n",
    "    print(f\"                 min={np.min(fc_first):.4f}, max={np.max(fc_first):.4f}\")\n",
    "\n",
    "print(f\"\\nFrequentist œÜ = {freq_results[ticker]['phi']:.4f}\")\n",
    "print(f\"Frequentist œÉ¬≤ = {freq_results[ticker]['sigma2']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bef01-50f7-41ff-8c85-070cb85162a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3-bayesian_final]",
   "language": "python",
   "name": "conda-env-Anaconda3-bayesian_final-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
